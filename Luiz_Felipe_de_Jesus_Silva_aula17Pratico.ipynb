{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "# Configurações do mundo\n",
        "TAMANHO = 5\n",
        "ORIGEM = (0, 0)\n",
        "ACOES = ['N', 'S', 'L', 'O', 'pegar', 'escalar']\n",
        "MOVIMENTOS = {'N': (-1, 0), 'S': (1, 0), 'L': (0, 1), 'O': (0, -1)}\n",
        "\n",
        "class MundoWumpus:\n",
        "    def __init__(self):\n",
        "        self.ouro = self._posicionar_aleatorio(evita=[ORIGEM])\n",
        "        self.wumpus = self._posicionar_aleatorio(evita=[ORIGEM, self.ouro])\n",
        "        self.abismos = self._criar_abismos(quantidade=4, evita=[ORIGEM, self.ouro, self.wumpus])\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.pos = ORIGEM\n",
        "        self.ouro_pego = False\n",
        "        self.passos = 0\n",
        "        self.max_passos = 100\n",
        "\n",
        "    def _criar_abismos(self, quantidade, evita):\n",
        "        abismos = set()\n",
        "        while len(abismos) < quantidade:\n",
        "            pos = self._posicionar_aleatorio(evita=evita)\n",
        "            evita.append(pos)\n",
        "            abismos.add(pos)\n",
        "        return abismos\n",
        "\n",
        "    def _posicionar_aleatorio(self, evita=[]):\n",
        "        while True:\n",
        "            pos = (random.randint(0, TAMANHO-1), random.randint(0, TAMANHO-1))\n",
        "            if pos not in evita:\n",
        "                return pos\n",
        "\n",
        "    def obter_estado(self):\n",
        "        return (self.pos[0], self.pos[1], self.ouro_pego)\n",
        "\n",
        "    def executar_acao(self, acao):\n",
        "        recompensa = -1\n",
        "        fim = False\n",
        "\n",
        "        if acao in MOVIMENTOS:\n",
        "            dx, dy = MOVIMENTOS[acao]\n",
        "            nx, ny = self.pos[0] + dx, self.pos[1] + dy\n",
        "\n",
        "            if 0 <= nx < TAMANHO and 0 <= ny < TAMANHO:\n",
        "                self.pos = (nx, ny)\n",
        "\n",
        "            if self.pos == self.wumpus:\n",
        "                recompensa -= 100\n",
        "                fim = True\n",
        "\n",
        "            if self.pos in self.abismos:\n",
        "                recompensa -= 100\n",
        "                fim = True\n",
        "\n",
        "        elif acao == 'pegar':\n",
        "            if self.pos == self.ouro and not self.ouro_pego:\n",
        "                self.ouro_pego = True\n",
        "                recompensa += 50\n",
        "\n",
        "        elif acao == 'escalar':\n",
        "            if self.pos == ORIGEM and self.ouro_pego:\n",
        "                recompensa += 50\n",
        "                fim = True\n",
        "\n",
        "        self.passos += 1\n",
        "        if self.passos >= self.max_passos:\n",
        "            fim = True\n",
        "\n",
        "        return recompensa, self.obter_estado(), fim\n",
        "\n",
        "class AgenteQLearning:\n",
        "    def __init__(self, ambiente, alpha=0.1, gamma=0.9, epsilon=0.2):\n",
        "        self.ambiente = ambiente\n",
        "        self.alpha = alpha\n",
        "        self.gamma = gamma\n",
        "        self.epsilon = epsilon\n",
        "        self.q_table = {}\n",
        "\n",
        "    def escolher_acao(self, estado):\n",
        "        if random.uniform(0, 1) < self.epsilon:\n",
        "            return random.choice(ACOES)\n",
        "        else:\n",
        "            q_valores = [self.q_table.get((estado, a), 0) for a in ACOES]\n",
        "            max_q = max(q_valores)\n",
        "            acoes_max = [a for a, q in zip(ACOES, q_valores) if q == max_q]\n",
        "            return random.choice(acoes_max)\n",
        "\n",
        "    def atualizar_Q(self, estado, acao, recompensa, novo_estado):\n",
        "        chave = (estado, acao)\n",
        "        q_atual = self.q_table.get(chave, 0)\n",
        "        max_q_novo = max([self.q_table.get((novo_estado, a), 0) for a in ACOES])\n",
        "        self.q_table[chave] = q_atual + self.alpha * (recompensa + self.gamma * max_q_novo - q_atual)\n",
        "\n",
        "def treinar(episodios):\n",
        "    mundo = MundoWumpus()\n",
        "    agente = AgenteQLearning(mundo)\n",
        "\n",
        "    print(f\"Ouro está em: {mundo.ouro}\")\n",
        "    print(f\"Wumpus está em: {mundo.wumpus}\")\n",
        "    print(f\"Abismos estão em: {mundo.abismos}\")\n",
        "\n",
        "    for ep in range(episodios):\n",
        "        mundo.reset()\n",
        "        estado = mundo.obter_estado()\n",
        "\n",
        "        while True:\n",
        "            acao = agente.escolher_acao(estado)\n",
        "            recompensa, novo_estado, fim = mundo.executar_acao(acao)\n",
        "            agente.atualizar_Q(estado, acao, recompensa, novo_estado)\n",
        "            estado = novo_estado\n",
        "            if fim:\n",
        "                break\n",
        "\n",
        "        if (ep + 1) % 1000 == 0:\n",
        "            print(f\"Episódio {ep+1} finalizado.\")\n",
        "\n",
        "    print(\"Treinamento concluído.\")\n",
        "\n",
        "    return agente, mundo\n",
        "\n",
        "def testar(agente, mundo):\n",
        "    agente.epsilon = 0\n",
        "    mundo.reset()\n",
        "    estado = mundo.obter_estado()\n",
        "    total_recompensa = 0\n",
        "    trajetoria = []\n",
        "\n",
        "    while True:\n",
        "        acao = agente.escolher_acao(estado)\n",
        "        recompensa, novo_estado, fim = mundo.executar_acao(acao)\n",
        "        total_recompensa += recompensa\n",
        "        trajetoria.append((mundo.pos, acao, recompensa, total_recompensa))\n",
        "        estado = novo_estado\n",
        "        if fim:\n",
        "            break\n",
        "\n",
        "    print(\"\\n--- Teste ---\")\n",
        "    for passo in trajetoria:\n",
        "        pos, acao, recompensa, acumulado = passo\n",
        "        print(f\"Posição: {pos} | Ação: {acao} | Recompensa: {recompensa} | Acumulado: {acumulado}\")\n",
        "\n",
        "    print(f\"\\nRecompensa total: {total_recompensa}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    agente_treinado, mundo = treinar(episodios=10000)\n",
        "    testar(agente_treinado, mundo)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wih-fcbIWGo3",
        "outputId": "16fbc60e-7ca8-4462-8945-f3017f836280"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ouro está em: (2, 2)\n",
            "Wumpus está em: (4, 1)\n",
            "Abismos estão em: {(1, 0), (0, 2), (3, 4), (1, 4)}\n",
            "Episódio 1000 finalizado.\n",
            "Episódio 2000 finalizado.\n",
            "Episódio 3000 finalizado.\n",
            "Episódio 4000 finalizado.\n",
            "Episódio 5000 finalizado.\n",
            "Episódio 6000 finalizado.\n",
            "Episódio 7000 finalizado.\n",
            "Episódio 8000 finalizado.\n",
            "Episódio 9000 finalizado.\n",
            "Episódio 10000 finalizado.\n",
            "Treinamento concluído.\n",
            "\n",
            "--- Teste ---\n",
            "Posição: (0, 1) | Ação: L | Recompensa: -1 | Acumulado: -1\n",
            "Posição: (1, 1) | Ação: S | Recompensa: -1 | Acumulado: -2\n",
            "Posição: (2, 1) | Ação: S | Recompensa: -1 | Acumulado: -3\n",
            "Posição: (2, 2) | Ação: L | Recompensa: -1 | Acumulado: -4\n",
            "Posição: (2, 2) | Ação: pegar | Recompensa: 49 | Acumulado: 45\n",
            "Posição: (2, 1) | Ação: O | Recompensa: -1 | Acumulado: 44\n",
            "Posição: (1, 1) | Ação: N | Recompensa: -1 | Acumulado: 43\n",
            "Posição: (0, 1) | Ação: N | Recompensa: -1 | Acumulado: 42\n",
            "Posição: (0, 0) | Ação: O | Recompensa: -1 | Acumulado: 41\n",
            "Posição: (0, 0) | Ação: escalar | Recompensa: 49 | Acumulado: 90\n",
            "\n",
            "Recompensa total: 90\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "# Configurações do mundo\n",
        "TAMANHO = 3\n",
        "ORIGEM = (0, 0)\n",
        "ACOES = ['N', 'S', 'L', 'O', 'pegar', 'escalar']\n",
        "MOVIMENTOS = {'N': (-1, 0), 'S': (1, 0), 'L': (0, 1), 'O': (0, -1)}\n",
        "\n",
        "class MundoWumpus:\n",
        "    def __init__(self):\n",
        "        self.ouro = self._posicionar_aleatorio(evita=[ORIGEM])\n",
        "        self.wumpus = self._posicionar_aleatorio(evita=[ORIGEM, self.ouro])\n",
        "        self.abismos = self._criar_abismos(quantidade=1, evita=[ORIGEM, self.ouro, self.wumpus])\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.pos = ORIGEM\n",
        "        self.ouro_pego = False\n",
        "        self.passos = 0\n",
        "        self.max_passos = 100\n",
        "\n",
        "    def _criar_abismos(self, quantidade, evita):\n",
        "        abismos = set()\n",
        "        while len(abismos) < quantidade:\n",
        "            pos = self._posicionar_aleatorio(evita=evita)\n",
        "            evita.append(pos)\n",
        "            abismos.add(pos)\n",
        "        return abismos\n",
        "\n",
        "    def _posicionar_aleatorio(self, evita=[]):\n",
        "        while True:\n",
        "            pos = (random.randint(0, TAMANHO-1), random.randint(0, TAMANHO-1))\n",
        "            if pos not in evita:\n",
        "                return pos\n",
        "\n",
        "    def obter_estado(self):\n",
        "        return (self.pos[0], self.pos[1], self.ouro_pego)\n",
        "\n",
        "    def executar_acao(self, acao):\n",
        "        recompensa = -1\n",
        "        fim = False\n",
        "\n",
        "        if acao in MOVIMENTOS:\n",
        "            dx, dy = MOVIMENTOS[acao]\n",
        "            nx, ny = self.pos[0] + dx, self.pos[1] + dy\n",
        "\n",
        "            if 0 <= nx < TAMANHO and 0 <= ny < TAMANHO:\n",
        "                self.pos = (nx, ny)\n",
        "\n",
        "            if self.pos == self.wumpus:\n",
        "                recompensa -= 100\n",
        "                fim = True\n",
        "\n",
        "            if self.pos in self.abismos:\n",
        "                recompensa -= 100\n",
        "                fim = True\n",
        "\n",
        "        elif acao == 'pegar':\n",
        "            if self.pos == self.ouro and not self.ouro_pego:\n",
        "                self.ouro_pego = True\n",
        "                recompensa += 50\n",
        "\n",
        "        elif acao == 'escalar':\n",
        "            if self.pos == ORIGEM and self.ouro_pego:\n",
        "                recompensa += 50\n",
        "                fim = True\n",
        "\n",
        "        self.passos += 1\n",
        "        if self.passos >= self.max_passos:\n",
        "            fim = True\n",
        "\n",
        "        return recompensa, self.obter_estado(), fim\n",
        "\n",
        "class AgenteQLearning:\n",
        "    def __init__(self, ambiente, alpha=0.1, gamma=0.9, epsilon=0.2):\n",
        "        self.ambiente = ambiente\n",
        "        self.alpha = alpha\n",
        "        self.gamma = gamma\n",
        "        self.epsilon = epsilon\n",
        "        self.q_table = {}\n",
        "\n",
        "    def escolher_acao(self, estado):\n",
        "        if random.uniform(0, 1) < self.epsilon:\n",
        "            return random.choice(ACOES)\n",
        "        else:\n",
        "            q_valores = [self.q_table.get((estado, a), 0) for a in ACOES]\n",
        "            max_q = max(q_valores)\n",
        "            acoes_max = [a for a, q in zip(ACOES, q_valores) if q == max_q]\n",
        "            return random.choice(acoes_max)\n",
        "\n",
        "    def atualizar_Q(self, estado, acao, recompensa, novo_estado):\n",
        "        chave = (estado, acao)\n",
        "        q_atual = self.q_table.get(chave, 0)\n",
        "        max_q_novo = max([self.q_table.get((novo_estado, a), 0) for a in ACOES])\n",
        "        self.q_table[chave] = q_atual + self.alpha * (recompensa + self.gamma * max_q_novo - q_atual)\n",
        "\n",
        "def treinar(episodios):\n",
        "    mundo = MundoWumpus()\n",
        "    agente = AgenteQLearning(mundo)\n",
        "\n",
        "    print(f\"Ouro está em: {mundo.ouro}\")\n",
        "    print(f\"Wumpus está em: {mundo.wumpus}\")\n",
        "    print(f\"Abismos estão em: {mundo.abismos}\")\n",
        "\n",
        "    for ep in range(episodios):\n",
        "        mundo.reset()\n",
        "        estado = mundo.obter_estado()\n",
        "\n",
        "        while True:\n",
        "            acao = agente.escolher_acao(estado)\n",
        "            recompensa, novo_estado, fim = mundo.executar_acao(acao)\n",
        "            agente.atualizar_Q(estado, acao, recompensa, novo_estado)\n",
        "            estado = novo_estado\n",
        "            if fim:\n",
        "                break\n",
        "\n",
        "        if (ep + 1) % 1000 == 0:\n",
        "            print(f\"Episódio {ep+1} finalizado.\")\n",
        "\n",
        "    print(\"Treinamento concluído.\")\n",
        "\n",
        "    print(\"\\n--- Q-Table ---\")\n",
        "    for chave in sorted(agente.q_table.keys()):\n",
        "        estado, acao = chave\n",
        "        valor_q = agente.q_table[chave]\n",
        "        print(f\"Estado: {estado} | Ação: {acao} | Q-valor: {valor_q:.2f}\")\n",
        "\n",
        "\n",
        "    return agente, mundo\n",
        "\n",
        "def testar(agente, mundo):\n",
        "    agente.epsilon = 0\n",
        "    mundo.reset()\n",
        "    estado = mundo.obter_estado()\n",
        "    total_recompensa = 0\n",
        "    trajetoria = []\n",
        "\n",
        "    while True:\n",
        "        acao = agente.escolher_acao(estado)\n",
        "        recompensa, novo_estado, fim = mundo.executar_acao(acao)\n",
        "        total_recompensa += recompensa\n",
        "        trajetoria.append((mundo.pos, acao, recompensa, total_recompensa))\n",
        "        estado = novo_estado\n",
        "        if fim:\n",
        "            break\n",
        "\n",
        "    print(\"\\n--- Teste ---\")\n",
        "    for passo in trajetoria:\n",
        "        pos, acao, recompensa, acumulado = passo\n",
        "        print(f\"Posição: {pos} | Ação: {acao} | Recompensa: {recompensa} | Acumulado: {acumulado}\")\n",
        "\n",
        "    print(f\"\\nRecompensa total: {total_recompensa}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    agente_treinado, mundo = treinar(episodios=10000)\n",
        "    testar(agente_treinado, mundo)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "91JRLVjtUZaz",
        "outputId": "dc044917-302d-417c-9ee5-5a69cd0a08dc"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ouro está em: (1, 2)\n",
            "Wumpus está em: (2, 1)\n",
            "Abismos estão em: {(1, 0)}\n",
            "Episódio 1000 finalizado.\n",
            "Episódio 2000 finalizado.\n",
            "Episódio 3000 finalizado.\n",
            "Episódio 4000 finalizado.\n",
            "Episódio 5000 finalizado.\n",
            "Episódio 6000 finalizado.\n",
            "Episódio 7000 finalizado.\n",
            "Episódio 8000 finalizado.\n",
            "Episódio 9000 finalizado.\n",
            "Episódio 10000 finalizado.\n",
            "Treinamento concluído.\n",
            "\n",
            "--- Q-Table ---\n",
            "Estado: (0, 0, False) | Ação: L | Q-valor: 265.60\n",
            "Estado: (0, 0, False) | Ação: N | Q-valor: 238.04\n",
            "Estado: (0, 0, False) | Ação: O | Q-valor: 238.04\n",
            "Estado: (0, 0, False) | Ação: S | Q-valor: -101.00\n",
            "Estado: (0, 0, False) | Ação: escalar | Q-valor: 238.04\n",
            "Estado: (0, 0, False) | Ação: pegar | Q-valor: 238.04\n",
            "Estado: (0, 0, True) | Ação: L | Q-valor: 395.00\n",
            "Estado: (0, 0, True) | Ação: N | Q-valor: 440.00\n",
            "Estado: (0, 0, True) | Ação: O | Q-valor: 440.00\n",
            "Estado: (0, 0, True) | Ação: S | Q-valor: -101.00\n",
            "Estado: (0, 0, True) | Ação: escalar | Q-valor: 490.00\n",
            "Estado: (0, 0, True) | Ação: pegar | Q-valor: 440.00\n",
            "Estado: (0, 1, False) | Ação: L | Q-valor: 296.22\n",
            "Estado: (0, 1, False) | Ação: N | Q-valor: 265.60\n",
            "Estado: (0, 1, False) | Ação: O | Q-valor: 238.04\n",
            "Estado: (0, 1, False) | Ação: S | Q-valor: 296.22\n",
            "Estado: (0, 1, False) | Ação: escalar | Q-valor: 265.60\n",
            "Estado: (0, 1, False) | Ação: pegar | Q-valor: 265.60\n",
            "Estado: (0, 1, True) | Ação: L | Q-valor: 354.50\n",
            "Estado: (0, 1, True) | Ação: N | Q-valor: 395.00\n",
            "Estado: (0, 1, True) | Ação: O | Q-valor: 440.00\n",
            "Estado: (0, 1, True) | Ação: S | Q-valor: 354.50\n",
            "Estado: (0, 1, True) | Ação: escalar | Q-valor: 395.00\n",
            "Estado: (0, 1, True) | Ação: pegar | Q-valor: 395.00\n",
            "Estado: (0, 2, False) | Ação: L | Q-valor: 295.79\n",
            "Estado: (0, 2, False) | Ação: N | Q-valor: 293.89\n",
            "Estado: (0, 2, False) | Ação: O | Q-valor: 264.82\n",
            "Estado: (0, 2, False) | Ação: S | Q-valor: 330.24\n",
            "Estado: (0, 2, False) | Ação: escalar | Q-valor: 294.53\n",
            "Estado: (0, 2, False) | Ação: pegar | Q-valor: 295.53\n",
            "Estado: (0, 2, True) | Ação: L | Q-valor: 354.50\n",
            "Estado: (0, 2, True) | Ação: N | Q-valor: 354.50\n",
            "Estado: (0, 2, True) | Ação: O | Q-valor: 395.00\n",
            "Estado: (0, 2, True) | Ação: S | Q-valor: 318.05\n",
            "Estado: (0, 2, True) | Ação: escalar | Q-valor: 354.50\n",
            "Estado: (0, 2, True) | Ação: pegar | Q-valor: 354.50\n",
            "Estado: (1, 1, False) | Ação: L | Q-valor: 330.24\n",
            "Estado: (1, 1, False) | Ação: N | Q-valor: 265.60\n",
            "Estado: (1, 1, False) | Ação: O | Q-valor: -101.00\n",
            "Estado: (1, 1, False) | Ação: S | Q-valor: -101.00\n",
            "Estado: (1, 1, False) | Ação: escalar | Q-valor: 296.22\n",
            "Estado: (1, 1, False) | Ação: pegar | Q-valor: 296.22\n",
            "Estado: (1, 1, True) | Ação: L | Q-valor: 318.05\n",
            "Estado: (1, 1, True) | Ação: N | Q-valor: 395.00\n",
            "Estado: (1, 1, True) | Ação: O | Q-valor: -92.94\n",
            "Estado: (1, 1, True) | Ação: S | Q-valor: -91.05\n",
            "Estado: (1, 1, True) | Ação: escalar | Q-valor: 340.78\n",
            "Estado: (1, 1, True) | Ação: pegar | Q-valor: 329.14\n",
            "Estado: (1, 2, False) | Ação: L | Q-valor: 330.24\n",
            "Estado: (1, 2, False) | Ação: N | Q-valor: 296.22\n",
            "Estado: (1, 2, False) | Ação: O | Q-valor: 296.22\n",
            "Estado: (1, 2, False) | Ação: S | Q-valor: 296.22\n",
            "Estado: (1, 2, False) | Ação: escalar | Q-valor: 330.24\n",
            "Estado: (1, 2, False) | Ação: pegar | Q-valor: 368.05\n",
            "Estado: (1, 2, True) | Ação: L | Q-valor: 318.05\n",
            "Estado: (1, 2, True) | Ação: N | Q-valor: 354.50\n",
            "Estado: (1, 2, True) | Ação: O | Q-valor: 354.50\n",
            "Estado: (1, 2, True) | Ação: S | Q-valor: 285.24\n",
            "Estado: (1, 2, True) | Ação: escalar | Q-valor: 318.05\n",
            "Estado: (1, 2, True) | Ação: pegar | Q-valor: 318.05\n",
            "Estado: (2, 2, False) | Ação: L | Q-valor: 219.61\n",
            "Estado: (2, 2, False) | Ação: N | Q-valor: 330.24\n",
            "Estado: (2, 2, False) | Ação: O | Q-valor: -52.69\n",
            "Estado: (2, 2, False) | Ação: S | Q-valor: 158.96\n",
            "Estado: (2, 2, False) | Ação: escalar | Q-valor: 237.80\n",
            "Estado: (2, 2, False) | Ação: pegar | Q-valor: 229.83\n",
            "Estado: (2, 2, True) | Ação: L | Q-valor: 181.68\n",
            "Estado: (2, 2, True) | Ação: N | Q-valor: 318.05\n",
            "Estado: (2, 2, True) | Ação: O | Q-valor: -77.89\n",
            "Estado: (2, 2, True) | Ação: S | Q-valor: 174.40\n",
            "Estado: (2, 2, True) | Ação: escalar | Q-valor: 170.57\n",
            "Estado: (2, 2, True) | Ação: pegar | Q-valor: 212.46\n",
            "\n",
            "--- Teste ---\n",
            "Posição: (0, 1) | Ação: L | Recompensa: -1 | Acumulado: -1\n",
            "Posição: (0, 2) | Ação: L | Recompensa: -1 | Acumulado: -2\n",
            "Posição: (1, 2) | Ação: S | Recompensa: -1 | Acumulado: -3\n",
            "Posição: (1, 2) | Ação: pegar | Recompensa: 49 | Acumulado: 46\n",
            "Posição: (0, 2) | Ação: N | Recompensa: -1 | Acumulado: 45\n",
            "Posição: (0, 1) | Ação: O | Recompensa: -1 | Acumulado: 44\n",
            "Posição: (0, 0) | Ação: O | Recompensa: -1 | Acumulado: 43\n",
            "Posição: (0, 0) | Ação: escalar | Recompensa: 49 | Acumulado: 92\n",
            "\n",
            "Recompensa total: 92\n"
          ]
        }
      ]
    }
  ]
}