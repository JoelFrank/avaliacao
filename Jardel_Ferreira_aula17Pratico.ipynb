{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NJAddschMMn8",
        "outputId": "43ad7e84-04f6-4f2b-c2f2-df891e50929b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iniciando o treinamento do agente...\n",
            "Posição do Wumpus: (1, 2)\n",
            "Posição do Ouro: (1, 0)\n",
            "Posições dos Poços: [(2, 2), (0, 1), (2, 1)]\n",
            "------------------------------\n",
            "Episódio 500/5000 - Recompensa Média (últimos 100): 1166.59 - Epsilon: 0.606\n",
            "Episódio 1000/5000 - Recompensa Média (últimos 100): 1767.36 - Epsilon: 0.368\n",
            "Episódio 1500/5000 - Recompensa Média (últimos 100): 1827.71 - Epsilon: 0.223\n",
            "Episódio 2000/5000 - Recompensa Média (últimos 100): 1967.68 - Epsilon: 0.135\n",
            "Episódio 2500/5000 - Recompensa Média (últimos 100): 1877.94 - Epsilon: 0.082\n",
            "Episódio 3000/5000 - Recompensa Média (últimos 100): 1997.88 - Epsilon: 0.050\n",
            "Episódio 3500/5000 - Recompensa Média (últimos 100): 1967.86 - Epsilon: 0.050\n",
            "Episódio 4000/5000 - Recompensa Média (últimos 100): 1967.92 - Epsilon: 0.050\n",
            "Episódio 4500/5000 - Recompensa Média (últimos 100): 1877.97 - Epsilon: 0.050\n",
            "Episódio 5000/5000 - Recompensa Média (últimos 100): 1937.91 - Epsilon: 0.050\n",
            "\n",
            "Treinamento concluído.\n",
            "Total de vitórias: 4540 de 5000 episódios (90.80%)\n",
            "\n",
            "--- Amostra da Q-Table Aprendida ---\n",
            "Estado (0, 0): Melhor ação -> right (Valores: {'left': 8990.0, 'up': -1001.0, 'down': 8990.0, 'right': 9990.0})\n",
            "Estado (1, 0): Melhor ação -> left (Valores: {'right': 8090.0, 'down': 8990.0, 'left': 9990.0, 'up': 8090.0})\n",
            "Estado (1, 1): Melhor ação -> down (Valores: {'right': -1001.0, 'down': 8990.0, 'up': -1001.0, 'left': -1001.0})\n",
            "Estado (2, 0): Melhor ação -> left (Valores: {'up': -1001.0, 'down': 8090.0, 'right': 7280.0, 'left': 8990.0})\n",
            "Estado (3, 0): Melhor ação -> left (Valores: {'right': 7278.83, 'down': 7279.52, 'left': 8090.0, 'up': 5867.17})\n",
            "Estado (3, 1): Melhor ação -> down (Valores: {'down': 7156.54, 'up': 312.75, 'right': 3513.03})\n",
            "Estado (3, 2): Melhor ação -> down (Valores: {'right': 312.52, 'down': 464.63, 'left': -750.75, 'up': -0.5})\n",
            "Estado (3, 3): Melhor ação -> down (Valores: {'down': 208.59})\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import defaultdict\n",
        "\n",
        "# --- Classe do Ambiente: Wumpus World ---\n",
        "class WumpusWorld:\n",
        "    def __init__(self, size=4):\n",
        "        self.size = size\n",
        "        self.agent_pos = (0, 0)\n",
        "        self.start_pos = (0, 0)\n",
        "\n",
        "        # Posiciona Wumpus, Ouro e Poços aleatoriamente, mas não na casa inicial\n",
        "        safe_positions = [(x, y) for x in range(size) for y in range(size) if (x, y) != self.start_pos]\n",
        "        random.shuffle(safe_positions)\n",
        "\n",
        "        self.wumpus_pos = safe_positions.pop()\n",
        "        self.gold_pos = safe_positions.pop()\n",
        "        self.pits_pos = random.sample(safe_positions, k=size-1) # Adiciona alguns poços\n",
        "\n",
        "        # Define as recompensas\n",
        "        self.reward_map = {\n",
        "            'gold': 1000,\n",
        "            'wumpus': -1000,\n",
        "            'pit': -1000,\n",
        "            'step': -1,\n",
        "            'win': 1000 # Recompensa por voltar à saída com o ouro\n",
        "        }\n",
        "        self.has_gold = False\n",
        "\n",
        "    def get_percepts(self, pos):\n",
        "        \"\"\" Retorna as percepções do agente na posição atual. \"\"\"\n",
        "        x, y = pos\n",
        "        percepts = {\n",
        "            'stench': False,\n",
        "            'breeze': False,\n",
        "            'glitter': False\n",
        "        }\n",
        "\n",
        "        # Verifica Brilho\n",
        "        if pos == self.gold_pos and not self.has_gold:\n",
        "            percepts['glitter'] = True\n",
        "\n",
        "        # Verifica Fedor (adjacente ao Wumpus)\n",
        "        wx, wy = self.wumpus_pos\n",
        "        if abs(x - wx) + abs(y - wy) == 1:\n",
        "            percepts['stench'] = True\n",
        "\n",
        "        # Verifica Brisa (adjacente a um poço)\n",
        "        for px, py in self.pits_pos:\n",
        "            if abs(x - px) + abs(y - py) == 1:\n",
        "                percepts['breeze'] = True\n",
        "\n",
        "        return percepts\n",
        "\n",
        "    def reset(self):\n",
        "        \"\"\" Reseta o ambiente para um novo episódio. \"\"\"\n",
        "        self.agent_pos = (0, 0)\n",
        "        self.has_gold = False\n",
        "        return self.agent_pos\n",
        "\n",
        "    def step(self, action):\n",
        "        \"\"\"\n",
        "        Executa uma ação do agente e retorna o novo estado, a recompensa e se o episódio terminou.\n",
        "        \"\"\"\n",
        "        x, y = self.agent_pos\n",
        "\n",
        "        # Movimenta o agente\n",
        "        if action == 'up':\n",
        "            y = min(self.size - 1, y + 1)\n",
        "        elif action == 'down':\n",
        "            y = max(0, y - 1)\n",
        "        elif action == 'left':\n",
        "            x = max(0, x - 1)\n",
        "        elif action == 'right':\n",
        "            x = min(self.size - 1, x + 1)\n",
        "\n",
        "        self.agent_pos = (x, y)\n",
        "        new_state = self.agent_pos\n",
        "\n",
        "        reward = self.reward_map['step']\n",
        "        done = False\n",
        "\n",
        "        # Verifica as consequências do movimento\n",
        "        if self.agent_pos == self.wumpus_pos:\n",
        "            reward += self.reward_map['wumpus']\n",
        "            done = True\n",
        "        elif self.agent_pos in self.pits_pos:\n",
        "            reward += self.reward_map['pit']\n",
        "            done = True\n",
        "        elif self.agent_pos == self.gold_pos and not self.has_gold:\n",
        "            reward += self.reward_map['gold']\n",
        "            self.has_gold = True # Agente pegou o ouro\n",
        "\n",
        "        # Verifica se o agente venceu (voltou à saída com o ouro)\n",
        "        if self.has_gold and self.agent_pos == self.start_pos:\n",
        "            reward += self.reward_map['win']\n",
        "            done = True\n",
        "\n",
        "        return new_state, reward, done\n",
        "\n",
        "# --- Classe do Agente com Q-Learning ---\n",
        "class QLearningAgent:\n",
        "    def __init__(self, actions, alpha=0.1, gamma=0.9, epsilon=0.9):\n",
        "        self.actions = actions\n",
        "        self.alpha = alpha     # Taxa de aprendizado\n",
        "        self.gamma = gamma     # Fator de desconto\n",
        "        self.epsilon = epsilon   # Taxa de exploração (vs. explotação)\n",
        "        self.epsilon_decay = 0.999 # Fator de decaimento do epsilon\n",
        "        self.epsilon_min = 0.05\n",
        "\n",
        "        # A Q-table é um dicionário aninhado: q_table[estado][acao] = valor\n",
        "        self.q_table = defaultdict(lambda: defaultdict(float))\n",
        "\n",
        "    def choose_action(self, state):\n",
        "        \"\"\" Escolhe uma ação usando a política epsilon-greedy. \"\"\"\n",
        "        if random.uniform(0, 1) < self.epsilon:\n",
        "            # Ação aleatória (Exploração)\n",
        "            return random.choice(self.actions)\n",
        "        else:\n",
        "            # Melhor ação conhecida (Explotação)\n",
        "            q_values = self.q_table[state]\n",
        "            if not q_values: # Se não houver valores para este estado, age aleatoriamente\n",
        "                return random.choice(self.actions)\n",
        "            return max(q_values, key=q_values.get)\n",
        "\n",
        "    def update_q_table(self, state, action, reward, next_state):\n",
        "        \"\"\" Atualiza a Q-table com a fórmula do Q-Learning. \"\"\"\n",
        "        old_value = self.q_table[state][action]\n",
        "\n",
        "        # Pega o valor Q máximo para o próximo estado\n",
        "        next_max_q = 0\n",
        "        if self.q_table[next_state]:\n",
        "            next_max_q = max(self.q_table[next_state].values())\n",
        "\n",
        "        # Fórmula do Q-Learning\n",
        "        new_value = old_value + self.alpha * (reward + self.gamma * next_max_q - old_value)\n",
        "        self.q_table[state][action] = new_value\n",
        "\n",
        "    def update_epsilon(self):\n",
        "        \"\"\" Reduz o epsilon para diminuir a exploração ao longo do tempo. \"\"\"\n",
        "        if self.epsilon > self.epsilon_min:\n",
        "            self.epsilon *= self.epsilon_decay\n",
        "\n",
        "# --- Loop Principal da Simulação ---\n",
        "if __name__ == \"__main__\":\n",
        "    # Configurações da simulação\n",
        "    env = WumpusWorld(size=4)\n",
        "    actions = ['up', 'down', 'left', 'right']\n",
        "    agent = QLearningAgent(actions=actions, alpha=0.5, gamma=0.9, epsilon=1.0)\n",
        "\n",
        "    num_episodes = 5000  #numero de iteração do treinamento\n",
        "    max_steps_per_episode = 1000 #quantidade de movimentos máxima em cada iteração\n",
        "    all_rewards = []\n",
        "    wins = 0\n",
        "\n",
        "    print(\"Iniciando o treinamento do agente...\")\n",
        "    print(f\"Posição do Wumpus: {env.wumpus_pos}\")\n",
        "    print(f\"Posição do Ouro: {env.gold_pos}\")\n",
        "    print(f\"Posições dos Poços: {env.pits_pos}\")\n",
        "    print(\"-\" * 30)\n",
        "\n",
        "    # Loop de treinamento por episódios\n",
        "    for episode in range(num_episodes):\n",
        "        state = env.reset()\n",
        "        total_reward = 0\n",
        "        done = False\n",
        "\n",
        "        for step in range(max_steps_per_episode):\n",
        "            action = agent.choose_action(state)\n",
        "            next_state, reward, done = env.step(action)\n",
        "\n",
        "            agent.update_q_table(state, action, reward, next_state)\n",
        "\n",
        "            state = next_state\n",
        "            total_reward += reward\n",
        "\n",
        "            if done:\n",
        "                # Verifica se o episódio terminou com vitória\n",
        "                if env.has_gold and env.agent_pos == env.start_pos:\n",
        "                    wins += 1\n",
        "                break\n",
        "\n",
        "        all_rewards.append(total_reward)\n",
        "        agent.update_epsilon() # Decai o epsilon após cada episódio\n",
        "\n",
        "        if (episode + 1) % 500 == 0:\n",
        "            print(f\"Episódio {episode + 1}/{num_episodes} - Recompensa Média (últimos 100): {sum(all_rewards[-100:])/100:.2f} - Epsilon: {agent.epsilon:.3f}\")\n",
        "\n",
        "    print(\"\\nTreinamento concluído.\")\n",
        "    print(f\"Total de vitórias: {wins} de {num_episodes} episódios ({wins/num_episodes*100:.2f}%)\")\n",
        "\n",
        "    # Exibindo a Q-Table aprendida (apenas alguns estados para visualização)\n",
        "    print(\"\\n--- Amostra da Q-Table Aprendida ---\")\n",
        "    for i in range(env.size):\n",
        "        for j in range(env.size):\n",
        "            state = (i, j)\n",
        "            if agent.q_table[state]:\n",
        "                best_action = max(agent.q_table[state], key=agent.q_table[state].get)\n",
        "                print(f\"Estado {state}: Melhor ação -> {best_action} (Valores: { {k: round(v, 2) for k,v in agent.q_table[state].items()} })\")\n"
      ]
    }
  ]
}