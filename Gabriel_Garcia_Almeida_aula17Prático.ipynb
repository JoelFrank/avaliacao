{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Algoritmo: Q-learning\n",
        "\n",
        "Código feito com o Google IA.\n",
        "\n",
        "É interessante que ao deixar um decaimento para o epsilon extremamente alto, ainda assim, após uma centena de iterações, o aprendizado já é o suficiente para vencer o cenário. Eu imagino que isso ocorra pois faz com que o algoritmo ao invés de explorar durante o treino, acaba fazendo como um algoritmo ACO e reforçando as ações positivas e penalizando as negativas e com o tempo passa a tomar somente as positivas.\n",
        "\n",
        "Também é interessante que o algoritmo só é capaz de encontrar a solução caso haja uma pequena penalidade para cada movimento tomado. Ao deixar como recompensa para o movimento como zero, o algoritmo as vezes chega na solução e as vezes não. Ao deixar positiva, ele faz o que qualquer um faria: fica indo de um lado para o outro infinitamente. Somente ao deixar uma penalidade, mesmo que muito pequena (-0.00000001) por movimento que o algoritmo alcança a solução ideal."
      ],
      "metadata": {
        "id": "tjDLKMQrHyv3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KW_Y5yQzb6sI",
        "outputId": "6484f0a1-52c1-4729-d346-fc49cc515e5f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episodio 100/5000 | Recompensa med. (ultimos 100): -1015.22 | epsilon: 0.905 | Vitórias (ultimos 100): 0\n",
            "Episodio 200/5000 | Recompensa med. (ultimos 100): -971.94 | epsilon: 0.819 | Vitórias (ultimos 100): 0\n",
            "Episodio 300/5000 | Recompensa med. (ultimos 100): -865.03 | epsilon: 0.741 | Vitórias (ultimos 100): 0\n",
            "Episodio 400/5000 | Recompensa med. (ultimos 100): -785.53 | epsilon: 0.670 | Vitórias (ultimos 100): 0\n",
            "Episodio 500/5000 | Recompensa med. (ultimos 100): -716.51 | epsilon: 0.606 | Vitórias (ultimos 100): 0\n",
            "Episodio 600/5000 | Recompensa med. (ultimos 100): -696.18 | epsilon: 0.549 | Vitórias (ultimos 100): 2\n",
            "Episodio 700/5000 | Recompensa med. (ultimos 100): -475.59 | epsilon: 0.496 | Vitórias (ultimos 100): 2\n",
            "Episodio 800/5000 | Recompensa med. (ultimos 100): -108.18 | epsilon: 0.449 | Vitórias (ultimos 100): 16\n",
            "Episodio 900/5000 | Recompensa med. (ultimos 100): 24.85 | epsilon: 0.406 | Vitórias (ultimos 100): 16\n",
            "Episodio 1000/5000 | Recompensa med. (ultimos 100): 288.72 | epsilon: 0.368 | Vitórias (ultimos 100): 27\n",
            "Episodio 1100/5000 | Recompensa med. (ultimos 100): 581.72 | epsilon: 0.333 | Vitórias (ultimos 100): 38\n",
            "Episodio 1200/5000 | Recompensa med. (ultimos 100): 638.86 | epsilon: 0.301 | Vitórias (ultimos 100): 38\n",
            "Episodio 1300/5000 | Recompensa med. (ultimos 100): 778.85 | epsilon: 0.272 | Vitórias (ultimos 100): 41\n",
            "Episodio 1400/5000 | Recompensa med. (ultimos 100): 1256.53 | epsilon: 0.246 | Vitórias (ultimos 100): 59\n",
            "Episodio 1500/5000 | Recompensa med. (ultimos 100): 1325.90 | epsilon: 0.223 | Vitórias (ultimos 100): 57\n",
            "Episodio 1600/5000 | Recompensa med. (ultimos 100): 1243.42 | epsilon: 0.202 | Vitórias (ultimos 100): 58\n",
            "Episodio 1700/5000 | Recompensa med. (ultimos 100): 1539.16 | epsilon: 0.183 | Vitórias (ultimos 100): 68\n",
            "Episodio 1800/5000 | Recompensa med. (ultimos 100): 1525.69 | epsilon: 0.165 | Vitórias (ultimos 100): 67\n",
            "Episodio 1900/5000 | Recompensa med. (ultimos 100): 1557.05 | epsilon: 0.149 | Vitórias (ultimos 100): 67\n",
            "Episodio 2000/5000 | Recompensa med. (ultimos 100): 1493.27 | epsilon: 0.135 | Vitórias (ultimos 100): 66\n",
            "Episodio 2100/5000 | Recompensa med. (ultimos 100): 1733.29 | epsilon: 0.122 | Vitórias (ultimos 100): 73\n",
            "Episodio 2200/5000 | Recompensa med. (ultimos 100): 2007.52 | epsilon: 0.111 | Vitórias (ultimos 100): 84\n",
            "Episodio 2300/5000 | Recompensa med. (ultimos 100): 1922.56 | epsilon: 0.100 | Vitórias (ultimos 100): 80\n",
            "Episodio 2400/5000 | Recompensa med. (ultimos 100): 2034.30 | epsilon: 0.091 | Vitórias (ultimos 100): 86\n",
            "Episodio 2500/5000 | Recompensa med. (ultimos 100): 2184.13 | epsilon: 0.082 | Vitórias (ultimos 100): 89\n",
            "Episodio 2600/5000 | Recompensa med. (ultimos 100): 1886.85 | epsilon: 0.074 | Vitórias (ultimos 100): 80\n",
            "Episodio 2700/5000 | Recompensa med. (ultimos 100): 2104.79 | epsilon: 0.067 | Vitórias (ultimos 100): 88\n",
            "Episodio 2800/5000 | Recompensa med. (ultimos 100): 2174.52 | epsilon: 0.061 | Vitórias (ultimos 100): 89\n",
            "Episodio 2900/5000 | Recompensa med. (ultimos 100): 2160.67 | epsilon: 0.055 | Vitórias (ultimos 100): 91\n",
            "Episodio 3000/5000 | Recompensa med. (ultimos 100): 2126.33 | epsilon: 0.050 | Vitórias (ultimos 100): 88\n",
            "Episodio 3100/5000 | Recompensa med. (ultimos 100): 2305.55 | epsilon: 0.045 | Vitórias (ultimos 100): 95\n",
            "Episodio 3200/5000 | Recompensa med. (ultimos 100): 2335.41 | epsilon: 0.041 | Vitórias (ultimos 100): 95\n",
            "Episodio 3300/5000 | Recompensa med. (ultimos 100): 2311.71 | epsilon: 0.037 | Vitórias (ultimos 100): 94\n",
            "Episodio 3400/5000 | Recompensa med. (ultimos 100): 2290.59 | epsilon: 0.033 | Vitórias (ultimos 100): 93\n",
            "Episodio 3500/5000 | Recompensa med. (ultimos 100): 2321.14 | epsilon: 0.030 | Vitórias (ultimos 100): 95\n",
            "Episodio 3600/5000 | Recompensa med. (ultimos 100): 2356.69 | epsilon: 0.027 | Vitórias (ultimos 100): 96\n",
            "Episodio 3700/5000 | Recompensa med. (ultimos 100): 2331.97 | epsilon: 0.025 | Vitórias (ultimos 100): 95\n",
            "Episodio 3800/5000 | Recompensa med. (ultimos 100): 2411.83 | epsilon: 0.022 | Vitórias (ultimos 100): 98\n",
            "Episodio 3900/5000 | Recompensa med. (ultimos 100): 2471.69 | epsilon: 0.020 | Vitórias (ultimos 100): 100\n",
            "Episodio 4000/5000 | Recompensa med. (ultimos 100): 2212.46 | epsilon: 0.018 | Vitórias (ultimos 100): 91\n",
            "Episodio 4100/5000 | Recompensa med. (ultimos 100): 2456.35 | epsilon: 0.017 | Vitórias (ultimos 100): 100\n",
            "Episodio 4200/5000 | Recompensa med. (ultimos 100): 2447.45 | epsilon: 0.015 | Vitórias (ultimos 100): 99\n",
            "Episodio 4300/5000 | Recompensa med. (ultimos 100): 2377.33 | epsilon: 0.014 | Vitórias (ultimos 100): 96\n",
            "Episodio 4400/5000 | Recompensa med. (ultimos 100): 2462.07 | epsilon: 0.012 | Vitórias (ultimos 100): 100\n",
            "Episodio 4500/5000 | Recompensa med. (ultimos 100): 2367.01 | epsilon: 0.011 | Vitórias (ultimos 100): 97\n",
            "Episodio 4600/5000 | Recompensa med. (ultimos 100): 2387.04 | epsilon: 0.010 | Vitórias (ultimos 100): 97\n",
            "Episodio 4700/5000 | Recompensa med. (ultimos 100): 2417.43 | epsilon: 0.009 | Vitórias (ultimos 100): 98\n",
            "Episodio 4800/5000 | Recompensa med. (ultimos 100): 2417.54 | epsilon: 0.008 | Vitórias (ultimos 100): 98\n",
            "Episodio 4900/5000 | Recompensa med. (ultimos 100): 2442.62 | epsilon: 0.007 | Vitórias (ultimos 100): 99\n",
            "Episodio 5000/5000 | Recompensa med. (ultimos 100): 2437.85 | epsilon: 0.007 | Vitórias (ultimos 100): 99\n",
            "Fim do treinamento\n",
            "\n",
            "--- Executando o agente treinado ---\n",
            "Começando na posição [1, 1]\n",
            "Tabuleiro inicial com o agente, abismos, Wumpus e ouro\n",
            ".   A   .   .  \n",
            ".   .   .   .  \n",
            ".   W   A   .  \n",
            "A   A   .   O  \n",
            "--------------------\n",
            "Ação 1: mover c | Estado=([3,0],CR:False,BS:True,BO:False,F:True,PO:False)\n",
            "Nova Posição: [2, 1], Recompensa da ação: -1, Total: -1\n",
            "\n",
            "Ação 2: atirar d | Estado=([2,0],CR:True,BS:False,BO:False,F:True,PO:False)\n",
            "Simulador: O Wumpus é morto e solta um grito!\n",
            "Nova Posição: [2, 1], Recompensa da ação: 489, Total: 488\n",
            "\n",
            "Ação 3: mover c | Estado=([2,0],CR:False,BS:False,BO:False,F:False,PO:False)\n",
            "Nova Posição: [3, 1], Recompensa da ação: -1, Total: 487\n",
            "\n",
            "Ação 4: mover d | Estado=([1,0],CR:False,BS:False,BO:False,F:False,PO:False)\n",
            "Nova Posição: [3, 2], Recompensa da ação: -1, Total: 486\n",
            "\n",
            "Ação 5: mover d | Estado=([1,1],CR:False,BS:True,BO:False,F:False,PO:False)\n",
            "Nova Posição: [3, 3], Recompensa da ação: -1, Total: 485\n",
            "\n",
            "Ação 6: mover d | Estado=([1,2],CR:False,BS:True,BO:False,F:False,PO:False)\n",
            "Nova Posição: [3, 4], Recompensa da ação: -1, Total: 484\n",
            "\n",
            "Ação 7: mover b | Estado=([1,3],CR:False,BS:False,BO:False,F:False,PO:False)\n",
            "Nova Posição: [2, 4], Recompensa da ação: -1, Total: 483\n",
            "\n",
            "Ação 8: mover b | Estado=([2,3],CR:False,BS:True,BO:False,F:False,PO:False)\n",
            "Nova Posição: [1, 4], Recompensa da ação: -1, Total: 482\n",
            "\n",
            "Ação 9: pegar ouro | Estado=([3,3],CR:False,BS:False,BO:True,F:False,PO:False)\n",
            "Simulador: Você pegou o ouro! Agora pode sair da caverna!\n",
            "Nova Posição: [1, 4], Recompensa da ação: 999, Total: 1481\n",
            "\n",
            "Ação 10: mover c | Estado=([3,3],CR:False,BS:False,BO:False,F:False,PO:True)\n",
            "Nova Posição: [2, 4], Recompensa da ação: -1, Total: 1480\n",
            "\n",
            "Ação 11: mover c | Estado=([2,3],CR:False,BS:True,BO:False,F:False,PO:True)\n",
            "Nova Posição: [3, 4], Recompensa da ação: -1, Total: 1479\n",
            "\n",
            "Ação 12: mover e | Estado=([1,3],CR:False,BS:False,BO:False,F:False,PO:True)\n",
            "Nova Posição: [3, 3], Recompensa da ação: -1, Total: 1478\n",
            "\n",
            "Ação 13: mover e | Estado=([1,2],CR:False,BS:True,BO:False,F:False,PO:True)\n",
            "Nova Posição: [3, 2], Recompensa da ação: -1, Total: 1477\n",
            "\n",
            "Ação 14: mover e | Estado=([1,1],CR:False,BS:True,BO:False,F:False,PO:True)\n",
            "Nova Posição: [3, 1], Recompensa da ação: -1, Total: 1476\n",
            "\n",
            "Ação 15: mover b | Estado=([1,0],CR:False,BS:False,BO:False,F:False,PO:True)\n",
            "Nova Posição: [2, 1], Recompensa da ação: -1, Total: 1475\n",
            "\n",
            "Ação 16: mover b | Estado=([2,0],CR:False,BS:False,BO:False,F:False,PO:True)\n",
            "Nova Posição: [1, 1], Recompensa da ação: -1, Total: 1474\n",
            "\n",
            "Ação 17: sair caverna | Estado=([3,0],CR:False,BS:True,BO:False,F:False,PO:True)\n",
            "Simulador: Saiu da caverna com ouro!\n",
            "Nova Posição: [1, 1], Recompensa da ação: 999, Total: 2473\n",
            "\n",
            "Fim do teste\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "import collections\n",
        "\n",
        "def preenche_wumpus(tabuleiro, tam_i, tam_j, tam_k, k, a, b):\n",
        "  # Fixando a posição\n",
        "  i = 2 # random.randint(0, tam_i - 1)\n",
        "  j = 1 # random.randint(0, tam_j - 1)\n",
        "  tabuleiro[i][j][k] = a\n",
        "  if(i - 1 >= 0): tabuleiro[i-1][j][k] = b\n",
        "  if(i + 1 < tam_i): tabuleiro[i+1][j][k] = b\n",
        "  if(j - 1 >= 0): tabuleiro[i][j-1][k] = b\n",
        "  if(j + 1 < tam_j): tabuleiro[i][j+1][k] = b\n",
        "  return (i, j)\n",
        "\n",
        "def edita_wumpus(tabuleiro, tam_i, tam_j, tam_k, i, j, k, a, b):\n",
        "  tabuleiro[i][j][k] = a\n",
        "  if(i - 1 >= 0): tabuleiro[i-1][j][k] = b\n",
        "  if(i + 1 < tam_i): tabuleiro[i+1][j][k] = b\n",
        "  if(j - 1 >= 0): tabuleiro[i][j-1][k] = b\n",
        "  if(j + 1 < tam_j): tabuleiro[i][j+1][k] = b\n",
        "\n",
        "def preenche_abismo(tabuleiro, tam_i, tam_j, tam_k, k, a, b, wumpus_pos):\n",
        "  # Fixando a posição\n",
        "  abismos = [(0,1), (2,2), (3,1)]\n",
        "\n",
        "  abismos_pos = []\n",
        "  aux = 0\n",
        "  while len(abismos_pos) < 3 and aux < len(abismos):\n",
        "      i, j = abismos[aux]\n",
        "      aux += 1\n",
        "\n",
        "      tabuleiro[i][j][k] = a\n",
        "      abismos_pos.append((i,j))\n",
        "      if(i - 1 >= 0 and tabuleiro[i-1][j][k] != 4):\n",
        "        tabuleiro[i-1][j][k] = b\n",
        "      if(i + 1 < tam_i and tabuleiro[i+1][j][k] != 4):\n",
        "        tabuleiro[i+1][j][k] = b\n",
        "      if(j - 1 >= 0 and tabuleiro[i][j-1][k] != 4):\n",
        "        tabuleiro[i][j-1][k] = b\n",
        "      if(j + 1 < tam_j and tabuleiro[i][j+1][k] != 4):\n",
        "        tabuleiro[i][j+1][k] = b\n",
        "  return abismos_pos\n",
        "\n",
        "\n",
        "def preenche_ouro(tabuleiro, tam_i, tam_j, tam_k, k, a, wumpus_pos, pit_coords_list):\n",
        "  # Fixando a posição\n",
        "  i = 3 # random.randint(0, tam_i - 1)\n",
        "  j = 3 # random.randint(0, tam_j - 1)\n",
        "\n",
        "  tabuleiro[i][j][k] = a\n",
        "  return (i,j)\n",
        "\n",
        "# Implementação do Q-learning\n",
        "tam_i = 4\n",
        "tam_j = 4\n",
        "\n",
        "alpha = 0.1\n",
        "gamma = 0.9\n",
        "epsilon = 1.0\n",
        "epsilon_decay = 0.999\n",
        "numero_episodios = 5000\n",
        "\n",
        "# Inicializando a tabela Q cheia de zeros\n",
        "q_table = collections.defaultdict(lambda: collections.defaultdict(float))\n",
        "\n",
        "# Definindo as ações\n",
        "ACTIONS = [\n",
        "    \"mover c\", \"mover b\", \"mover e\", \"mover d\",\n",
        "    \"atirar c\", \"atirar b\", \"atirar e\", \"atirar d\",\n",
        "    \"pegar ouro\",\n",
        "    \"sair caverna\"\n",
        "]\n",
        "\n",
        "def informacao_estado(pos_i, pos_j, percepcoes, tem_flecha, pegou_ouro):\n",
        "    return (pos_i, pos_j, percepcoes[0], percepcoes[1], percepcoes[2], tem_flecha, pegou_ouro)\n",
        "\n",
        "def possiveis_acoes(tem_flecha, pegou_ouro, pos_i, pos_j):\n",
        "    # Sempre é possível fazer\n",
        "    available = [\"mover c\", \"mover b\", \"mover e\", \"mover d\", \"pegar ouro\"]\n",
        "    # Caso tenha flecha\n",
        "    if tem_flecha:\n",
        "        available.extend([\"atirar c\", \"atirar b\", \"atirar e\", \"atirar d\"])\n",
        "    # Caso possa sair da caverna\n",
        "    if pegou_ouro and pos_i == tam_i - 1 and pos_j == 0:\n",
        "        available.append(\"sair caverna\")\n",
        "    return available\n",
        "\n",
        "\n",
        "def define_acao(estado, tem_flecha, pegou_ouro, pos_i_atual, pos_j_atual, epsilon_atual):\n",
        "    possiveis_acoes1 = possiveis_acoes(tem_flecha, pegou_ouro, pos_i_atual, pos_j_atual)\n",
        "\n",
        "    # Verifica se explora ou usa a tabela\n",
        "    if random.uniform(0, 1) < epsilon_atual:\n",
        "        return random.choice(possiveis_acoes1)\n",
        "    else:\n",
        "        valores_prox = q_table[estado]\n",
        "        max_q = -float('inf')\n",
        "        melhor_escolha = None\n",
        "        for acao in possiveis_acoes1:\n",
        "            if valores_prox[acao] > max_q:\n",
        "                max_q = valores_prox[acao]\n",
        "                melhor_escolha = acao\n",
        "        return melhor_escolha\n",
        "\n",
        "\n",
        "def atualizar_tabela(estado, acao, recompensa, prox_estado, done, next_tem_flecha, next_pegou_ouro, next_pos_i, next_pos_j):\n",
        "    valor_antigo = q_table[estado][acao]\n",
        "\n",
        "    # Determina o valor máximo para o próximo estado\n",
        "    next_max_q = 0.0\n",
        "    if not done:\n",
        "        next_available_actions = possiveis_acoes(next_tem_flecha, next_pegou_ouro, next_pos_i, next_pos_j)\n",
        "        if next_available_actions:\n",
        "            q_values_for_next_state = q_table[prox_estado]\n",
        "            max_val = -float('inf')\n",
        "            for act in next_available_actions:\n",
        "                if q_values_for_next_state[act] > max_val:\n",
        "                    max_val = q_values_for_next_state[act]\n",
        "            if max_val != -float('inf'):\n",
        "                 next_max_q = max_val\n",
        "\n",
        "    # Formula de atualização\n",
        "    novo_valor = valor_antigo + alpha * (recompensa + gamma * next_max_q - valor_antigo)\n",
        "    q_table[estado][acao] = novo_valor\n",
        "\n",
        "# Loop de treinamento e teste\n",
        "if True:\n",
        "    tam_k = 3  # Dimensões: 0:Wumpus, 1:Abismo, 2:Ouro\n",
        "\n",
        "    recompensa_episodio = []\n",
        "    num_vitorias = 0\n",
        "\n",
        "    epsilon_atual = epsilon\n",
        "\n",
        "    for episodio in range(numero_episodios):\n",
        "        # Reseta o tabuleiro para cada episódio\n",
        "        tabuleiro = [[[0 for _ in range(tam_k)] for _ in range(tam_j)] for _ in range(tam_i)]\n",
        "        # Wumpus = 2, Cheiro ruim = 3\n",
        "        wumpus_i, wumpus_j = preenche_wumpus(tabuleiro, tam_i, tam_j, tam_k, 0, 2, 3)\n",
        "        wumpus_pos_original = (wumpus_i, wumpus_j)\n",
        "        # Abismo = 4, Brisa = 5\n",
        "        pit_coords_list = preenche_abismo(tabuleiro, tam_i, tam_j, tam_k, 1, 4, 5, wumpus_pos_original)\n",
        "        # Ouro = 6\n",
        "        ouro_i, ouro_j = preenche_ouro(tabuleiro, tam_i, tam_j, tam_k, 2, 6, wumpus_pos_original, pit_coords_list)\n",
        "        ouro_pos_original = (ouro_i, ouro_j)\n",
        "\n",
        "        # Estado inicial e variáveis\n",
        "        pos_i = tam_i - 1\n",
        "        pos_j = 0\n",
        "        flecha = True\n",
        "        pegou_ouro = False\n",
        "        wumpus_morto = False\n",
        "\n",
        "        # Estatísticas do episódio\n",
        "        recompensa_atual = 0\n",
        "        done = False\n",
        "        iteracao = 0\n",
        "        iteracoes_por_episodio = 100\n",
        "\n",
        "        while not done and iteracao < iteracoes_por_episodio:\n",
        "            # Determina o estado\n",
        "            cheiro_ruim = (tabuleiro[pos_i][pos_j][0] == 3)\n",
        "            brisa = (tabuleiro[pos_i][pos_j][1] == 5)\n",
        "            brilho = (tabuleiro[pos_i][pos_j][2] == 6)\n",
        "            percepcoes_atual = (cheiro_ruim, brisa, brilho)\n",
        "            state = informacao_estado(pos_i, pos_j, percepcoes_atual, flecha, pegou_ouro)\n",
        "\n",
        "            # Tomada da ação\n",
        "            acao = define_acao(state, flecha, pegou_ouro, pos_i, pos_j, epsilon_atual)\n",
        "\n",
        "            # Guarda os valores antigos\n",
        "            pos_i_velha, pos_j_velha = pos_i, pos_j\n",
        "            flecha_velha, pegou_ouro_velha = flecha, pegou_ouro\n",
        "\n",
        "            # Simula e recebe a recompensa\n",
        "            recompensa_teste = -1  # Custo base\n",
        "            pos_i_prox, pos_j_prox = pos_i, pos_j\n",
        "\n",
        "            if acao.startswith(\"mover\"):\n",
        "                direcao = acao.split(\" \")[1]\n",
        "                if direcao == \"d\": pos_j_prox += 1\n",
        "                elif direcao == \"e\": pos_j_prox -= 1\n",
        "                elif direcao == \"c\": pos_i_prox -= 1\n",
        "                elif direcao == \"b\": pos_i_prox += 1\n",
        "\n",
        "                # Verifica se está nos limites do tabuleiro\n",
        "                if not (0 <= pos_i_prox < tam_i and 0 <= pos_j_prox < tam_j):\n",
        "                    # Colisão com a parede\n",
        "                    recompensa_teste -= 10\n",
        "                    pos_i_prox, pos_j_prox = pos_i, pos_j\n",
        "                else:\n",
        "                    pos_i, pos_j = pos_i_prox, pos_j_prox\n",
        "\n",
        "            elif acao.startswith(\"atirar\") and flecha:\n",
        "                flecha = False\n",
        "                recompensa_teste -= 10 # Custo de atirar\n",
        "\n",
        "                target_i, target_j = pos_i, pos_j\n",
        "                direcao = acao.split(\" \")[1]\n",
        "                if direcao == \"d\": target_j += 1\n",
        "                elif direcao == \"e\": target_j -= 1\n",
        "                elif direcao == \"c\": target_i -= 1\n",
        "                elif direcao == \"b\": target_i += 1\n",
        "\n",
        "                if 0 <= target_i < tam_i and 0 <= target_j < tam_j:\n",
        "                    if tabuleiro[target_i][target_j][0] == 2: # Acertou\n",
        "                        recompensa_teste += 500\n",
        "                        wumpus_morto = True\n",
        "                        # Retira o Wumpus do tabuleiro\n",
        "                        edita_wumpus(tabuleiro, tam_i, tam_j, tam_k, target_i, target_j, 0, 0, 0)\n",
        "\n",
        "            elif acao == \"pegar ouro\":\n",
        "                if tabuleiro[pos_i][pos_j][2] == 6 and not pegou_ouro:\n",
        "                    pegou_ouro = True\n",
        "                    recompensa_teste += 1000\n",
        "                    tabuleiro[pos_i][pos_j][2] = 0\n",
        "                elif pegou_ouro:\n",
        "                    recompensa_teste -= 5\n",
        "                else:\n",
        "                    recompensa_teste -= 5\n",
        "\n",
        "            elif acao == \"sair caverna\":\n",
        "                if pos_i == tam_i - 1 and pos_j == 0 and pegou_ouro:\n",
        "                    recompensa_teste += 1000\n",
        "                    done = True\n",
        "                    num_vitorias +=1\n",
        "                else:\n",
        "                    recompensa_teste -= 10\n",
        "\n",
        "            # Verifica mortes\n",
        "            if not done:\n",
        "                if tabuleiro[pos_i][pos_j][0] == 2 and not wumpus_morto:\n",
        "                    recompensa_teste -= 1000\n",
        "                    done = True\n",
        "                elif tabuleiro[pos_i][pos_j][1] == 4:\n",
        "                    recompensa_teste -= 1000\n",
        "                    done = True\n",
        "\n",
        "            recompensa_atual += recompensa_teste\n",
        "\n",
        "            # Observa o próximo estado\n",
        "            cheiro_ruim_prox = (tabuleiro[pos_i][pos_j][0] == 3)\n",
        "            brisa_prox = (tabuleiro[pos_i][pos_j][1] == 5)\n",
        "            brilho_prox = (tabuleiro[pos_i][pos_j][2] == 6)\n",
        "            percepcoes_proximo = (cheiro_ruim_prox, brisa_prox, brilho_prox)\n",
        "\n",
        "            prox_estado = informacao_estado(pos_i, pos_j, percepcoes_proximo, flecha, pegou_ouro)\n",
        "\n",
        "            # Atualiza a tabela\n",
        "            estado_para_atualizar = informacao_estado(pos_i_velha, pos_j_velha, percepcoes_atual, flecha_velha, pegou_ouro_velha)\n",
        "            atualizar_tabela(estado_para_atualizar, acao, recompensa_teste, prox_estado, done, flecha, pegou_ouro, pos_i, pos_j)\n",
        "\n",
        "            iteracao += 1\n",
        "\n",
        "        recompensa_episodio.append(recompensa_atual)\n",
        "        epsilon_atual = epsilon_atual * epsilon_decay\n",
        "\n",
        "        if (episodio + 1) % 100 == 0:\n",
        "            recompensa_media = sum(recompensa_episodio[-100:]) / 100\n",
        "            print(f\"Episodio {episodio + 1}/{numero_episodios} | Recompensa med. (ultimos 100): {recompensa_media:.2f} | epsilon: {epsilon_atual:.3f} | Vitórias (ultimos 100): {num_vitorias}\")\n",
        "            num_vitorias = 0\n",
        "\n",
        "    print(\"Fim do treinamento\")\n",
        "\n",
        "    # Resultado\n",
        "    print(\"\\n--- Executando o agente treinado ---\")\n",
        "    # Prepara o ambiente novamente\n",
        "    tabuleiro = [[[0 for _ in range(tam_k)] for _ in range(tam_j)] for _ in range(tam_i)]\n",
        "    wumpus_i, wumpus_j = preenche_wumpus(tabuleiro, tam_i, tam_j, tam_k, 0, 2, 3)\n",
        "    pit_coords_list = preenche_abismo(tabuleiro, tam_i, tam_j, tam_k, 1, 4, 5, (wumpus_i, wumpus_j))\n",
        "    ouro_i, ouro_j = preenche_ouro(tabuleiro, tam_i, tam_j, tam_k, 2, 6, (wumpus_i, wumpus_j), pit_coords_list)\n",
        "\n",
        "    pos_i = tam_i - 1\n",
        "    pos_j = 0\n",
        "    flecha = True\n",
        "    pegou_ouro = False\n",
        "    wumpus_morto_teste = False\n",
        "    done_teste = False\n",
        "    passos = 0\n",
        "    maximo_passos = 50\n",
        "    recompensa_teste = 0\n",
        "\n",
        "    print(f\"Começando na posição [{tam_i-pos_i}, {pos_j+1}]\")\n",
        "    print(\"Tabuleiro inicial com o agente, abismos, Wumpus e ouro\")\n",
        "    for r_idx, row_val in enumerate(tabuleiro):\n",
        "        display_row = []\n",
        "        for c_idx, cell_val in enumerate(row_val):\n",
        "            s = \"\"\n",
        "            if cell_val[0] == 2: s += \"W\"\n",
        "            if cell_val[1] == 4: s += \"A\"\n",
        "            if cell_val[2] == 6: s += \"O\"\n",
        "            if not s: s = \".\"\n",
        "            if r_idx == pos_i and c_idx == pos_j: s = \"A\"\n",
        "            display_row.append(s.ljust(3))\n",
        "        print(\" \".join(display_row))\n",
        "    print(\"-\" * 20)\n",
        "\n",
        "\n",
        "    while not done_teste and passos < maximo_passos:\n",
        "        cheiro_ruim = (tabuleiro[pos_i][pos_j][0] == 3)\n",
        "        brisa = (tabuleiro[pos_i][pos_j][1] == 5)\n",
        "        brilho = (tabuleiro[pos_i][pos_j][2] == 6)\n",
        "        percepcoes_atual = (cheiro_ruim, brisa, brilho)\n",
        "        state = informacao_estado(pos_i, pos_j, percepcoes_atual, flecha, pegou_ouro)\n",
        "\n",
        "        acao = define_acao(state, flecha, pegou_ouro, pos_i, pos_j, 0.0)\n",
        "\n",
        "        print(f\"Ação {passos+1}: {acao} | Estado=([{pos_i},{pos_j}],CR:{cheiro_ruim},BS:{brisa},BO:{brilho},F:{flecha},PO:{pegou_ouro})\")\n",
        "\n",
        "        recompensa_acao = -1\n",
        "        if acao.startswith(\"mover\"):\n",
        "            direcao = acao.split(\" \")[1]\n",
        "            pos_i_prox_teste, pos_j_prox_teste = pos_i, pos_j\n",
        "            if direcao == \"d\": pos_j_prox_teste += 1\n",
        "            elif direcao == \"e\": pos_j_prox_teste -= 1\n",
        "            elif direcao == \"c\": pos_i_prox_teste -= 1\n",
        "            elif direcao == \"b\": pos_i_prox_teste += 1\n",
        "\n",
        "            if not (0 <= pos_i_prox_teste < tam_i and 0 <= pos_j_prox_teste < tam_j):\n",
        "                print(\"Simulador: Bateu na parede!\")\n",
        "                recompensa_acao -=10\n",
        "            else:\n",
        "                pos_i, pos_j = pos_i_prox_teste, pos_j_prox_teste\n",
        "\n",
        "        elif acao.startswith(\"atirar\") and flecha:\n",
        "            flecha = False\n",
        "            recompensa_acao -=10\n",
        "            target_i, target_j = pos_i, pos_j\n",
        "            direcao = acao.split(\" \")[1]\n",
        "            if direcao == \"d\": target_j += 1\n",
        "            elif direcao == \"e\": target_j -= 1\n",
        "            elif direcao == \"c\": target_i -= 1\n",
        "            elif direcao == \"b\": target_j += 1\n",
        "\n",
        "            if 0 <= target_i < tam_i and 0 <= target_j < tam_j:\n",
        "                if tabuleiro[target_i][target_j][0] == 2 and not wumpus_morto_teste:\n",
        "                    print(\"Simulador: O Wumpus é morto e solta um grito!\")\n",
        "                    wumpus_morto_teste = True\n",
        "                    recompensa_acao += 500\n",
        "                    edita_wumpus(tabuleiro, tam_i, tam_j, tam_k, target_i, target_j, 0, 0, 0)\n",
        "                else:\n",
        "                    print(\"Simulador: Você ouve o som da flecha caindo no chão...\")\n",
        "            else:\n",
        "                 print(\"Simulador: Você ouve o som da flecha caindo no chão...\")\n",
        "\n",
        "\n",
        "        elif acao == \"pegar ouro\":\n",
        "            if tabuleiro[pos_i][pos_j][2] == 6 and not pegou_ouro:\n",
        "                pegou_ouro = True\n",
        "                tabuleiro[pos_i][pos_j][2] = 0\n",
        "                print(\"Simulador: Você pegou o ouro! Agora pode sair da caverna!\")\n",
        "                recompensa_acao +=1000\n",
        "            elif pegou_ouro: print(\"Simulador: Você já está com o ouro.\")\n",
        "            else: print(\"Simulador: O ouro não está aqui... Procure pelo seu brilho.\")\n",
        "\n",
        "        elif acao == \"sair caverna\":\n",
        "            if pos_i == tam_i - 1 and pos_j == 0 and pegou_ouro:\n",
        "                print(\"Simulador: Saiu da caverna com ouro!\")\n",
        "                done_teste = True\n",
        "                recompensa_acao +=1000\n",
        "            else:\n",
        "                print(\"Simulador: Para sair é preciso ter pego o ouro e estar na posição [1, 1].\")\n",
        "                recompensa_acao -=10\n",
        "\n",
        "        # Verifica morte\n",
        "        if not done_teste:\n",
        "            if tabuleiro[pos_i][pos_j][0] == 2 and not wumpus_morto_teste:\n",
        "                print(f\"Simulador: MORTO PELO WUMPUS em [{tam_i-pos_i}, {pos_j+1}]\")\n",
        "                done_teste = True\n",
        "                recompensa_acao -=1000\n",
        "            elif tabuleiro[pos_i][pos_j][1] == 4:\n",
        "                print(f\"Simulador: CAIU NO ABISMO em [{tam_i-pos_i}, {pos_j+1}]\")\n",
        "                done_teste = True\n",
        "                recompensa_acao -=1000\n",
        "\n",
        "        recompensa_teste += recompensa_acao\n",
        "        passos += 1\n",
        "        print(f\"Nova Posição: [{tam_i-pos_i}, {pos_j+1}], Recompensa da ação: {recompensa_acao}, Total: {recompensa_teste}\\n\")\n",
        "        if done_teste:\n",
        "            print(\"Fim do teste\")\n",
        "\n",
        "    if not done_teste:\n",
        "        print(\"Maximo de ações alcançado\")"
      ]
    }
  ]
}