{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1ty22KnXXIZOzY--OgkCtwSa4hwsQ8B9n","timestamp":1750707516868}],"authorship_tag":"ABX9TyPjagmb0uhRaG8+xh32hQw6"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["import random\n","import numpy as np\n","import time\n","\n","class Celula:\n","    def __init__(self):\n","        self.tem_abismo = False\n","        self.tem_wumpus = False\n","        self.tem_ouro = False\n","        self.tem_brisa = False\n","        self.tem_fedor = False\n","\n","class MundoWumpus:\n","    def __init__(self, tamanho=4):\n","        self.tamanho = tamanho\n","        self.agente_e_bot = True\n","        self.posicoes_abismos = [(3, 1), (4, 4), (1, 3)]\n","        self.wumpus_pos_inicial = (3, 4)\n","        self.ouro_pos_inicial = (2, 3)\n","        self.grade = {}\n","        self.reiniciar()\n","\n","    def _obter_tupla_estado_atual(self):\n","        return (self.pos_agente, self.ouro_foi_pego, self.wumpus_esta_vivo)\n","\n","    def exibir_mapa(self):\n","        print('' * (self.tamanho + 2))\n","        for y in range(self.tamanho, 0, -1):\n","            linha = ''\n","            for x in range(1, self.tamanho + 1):\n","                pos = (x, y)\n","                if pos == self.pos_agente:\n","                    linha += ''\n","                elif self.grade[pos].tem_abismo:\n","                    linha += ''\n","                elif pos == self.pos_wumpus and self.wumpus_esta_vivo:\n","                    linha += ''\n","                elif pos == self.pos_wumpus and not self.wumpus_esta_vivo:\n","                    linha += ''\n","                elif pos == self.pos_ouro and not self.ouro_foi_pego:\n","                    linha += ''\n","                else:\n","                    linha += '锔'\n","            linha += ''\n","            print(linha)\n","        print('' * (self.tamanho + 2))\n","\n","    def atualizar_percepcoes(self):\n","        for celula in self.grade.values():\n","            celula.tem_brisa = celula.tem_fedor = False\n","\n","        for pos, celula in self.grade.items():\n","            if celula.tem_abismo:\n","                for dx, dy in [(-1, 0), (1, 0), (0, -1), (0, 1)]:\n","                    adjacente = (pos[0] + dx, pos[1] + dy)\n","                    if adjacente in self.grade:\n","                        self.grade[adjacente].tem_brisa = True\n","\n","        if self.wumpus_esta_vivo:\n","            for dx, dy in [(-1, 0), (1, 0), (0, -1), (0, 1)]:\n","                adjacente = (self.pos_wumpus[0] + dx, self.pos_wumpus[1] + dy)\n","                if adjacente in self.grade:\n","                    self.grade[adjacente].tem_fedor = True\n","\n","    def reiniciar(self):\n","        self.pos_agente = (1, 1)\n","        self.ouro_foi_pego = False\n","        self.wumpus_esta_vivo = True\n","        self.pos_wumpus = self.wumpus_pos_inicial\n","        self.pos_ouro = self.ouro_pos_inicial\n","        self.flecha_disponivel = True\n","        self.pontuacao = 0\n","\n","        self.grade = {(x, y): Celula() for x in range(1, self.tamanho + 1) for y in range(1, self.tamanho + 1)}\n","        for pos in self.posicoes_abismos:\n","            self.grade[pos].tem_abismo = True\n","        self.grade[self.pos_wumpus].tem_wumpus = True\n","        self.grade[self.pos_ouro].tem_ouro = True\n","\n","        self.atualizar_percepcoes()\n","        return self._obter_tupla_estado_atual()\n","\n","    def executar_passo(self, acao):\n","        recompensa = -1\n","        terminou = False\n","        info = \"\"\n","\n","        if acao < 4:\n","            movimentos = {0: (0, 1), 1: (-1, 0), 2: (0, -1), 3: (1, 0)}\n","            dx, dy = movimentos[acao]\n","            new_pos = (self.pos_agente[0] + dx, self.pos_agente[1] + dy)\n","\n","            if 1 <= new_pos[0] <= self.tamanho and 1 <= new_pos[1] <= self.tamanho:\n","                self.pos_agente = new_pos\n","                if self.grade[new_pos].tem_abismo or (new_pos == self.pos_wumpus and self.wumpus_esta_vivo):\n","                    recompensa = -1000\n","                    terminou = True\n","                    info = \"Morreu\"\n","                else:\n","                    info = f\"Moveu para {new_pos}\"\n","            else:\n","                info = \"Bateu na parede\"\n","\n","        elif acao == 4:\n","            recompensa = -10\n","            if self.flecha_disponivel:\n","                self.flecha_disponivel = False\n","                for dx, dy in [(-1, 0), (1, 0), (0, -1), (0, 1)]:\n","                    alvo = (self.pos_agente[0] + dx, self.pos_agente[1] + dy)\n","                    if alvo == self.pos_wumpus:\n","                        self.wumpus_esta_vivo = False\n","                        self.atualizar_percepcoes()\n","                        recompensa += 100\n","                        info = \"Matou o Wumpus\"\n","                        break\n","                else:\n","                    info = \"Flecha perdida\"\n","            else:\n","                info = \"Sem flechas\"\n","\n","        elif acao == 5:\n","            if self.pos_agente == self.pos_ouro and not self.ouro_foi_pego:\n","                self.ouro_foi_pego = True\n","                recompensa += 1000\n","                info = \"Pegou o ouro\"\n","            else:\n","                info = \"Nada para pegar\"\n","\n","        if not self.wumpus_esta_vivo and self.ouro_foi_pego and self.pos_agente == (1, 1):\n","            recompensa += 1000\n","            terminou = True\n","            info = \"Vit贸ria!\"\n","\n","        self.pontuacao += recompensa\n","        if self.pontuacao < -150:\n","            terminou = True\n","            info = \"Limite de passos excedido.\"\n","\n","        return self._obter_tupla_estado_atual(), recompensa, terminou, info\n","\n","class AgenteQLearning:\n","    def __init__(self, tamanho, acoes, taxa_aprendizado=0.1, fator_desconto=0.95, epsilon=0.9):\n","        self.tabela_q = np.zeros((tamanho + 1, tamanho + 1, 2, 2, acoes))\n","        self.taxa_aprendizado = taxa_aprendizado\n","        self.fator_desconto = fator_desconto\n","        self.epsilon = epsilon\n","        self.acoes = range(acoes)\n","\n","    def _obter_indices_estado(self, tupla_estado):\n","        pos, tem_ouro, wumpus_vivo = tupla_estado\n","        return pos[0], pos[1], int(tem_ouro), int(wumpus_vivo)\n","\n","    def escolher_acao(self, estado):\n","        if random.uniform(0, 1) < self.epsilon:\n","            return random.choice(self.acoes)\n","        else:\n","            indices_estado = self._obter_indices_estado(estado)\n","            return np.argmax(self.tabela_q[indices_estado])\n","\n","    def aprender(self, estado, acao, recompensa, prox_estado):\n","        indices_estado = self._obter_indices_estado(estado)\n","        indices_prox_estado = self._obter_indices_estado(prox_estado)\n","\n","        fatia_tabela_q_para_acao = indices_estado + (acao,)\n","        previsao = self.tabela_q[fatia_tabela_q_para_acao]\n","\n","        recompensa_futura = np.max(self.tabela_q[indices_prox_estado])\n","        alvo = recompensa + self.fator_desconto * recompensa_futura\n","\n","        self.tabela_q[fatia_tabela_q_para_acao] += self.taxa_aprendizado * (alvo - previsao)\n","\n","def treinar_agente(episodios):\n","    env = MundoWumpus()\n","    agente = AgenteQLearning(tamanho=env.tamanho, acoes=6)\n","    recompensas = []\n","\n","    taxa_decaimento_epsilon = 0.99995\n","    epsilon_minimo = 0.05\n","\n","    for episodio in range(episodios):\n","        estado = env.reiniciar()\n","        terminou = False\n","        recompensa_total = 0\n","\n","        while not terminou:\n","            acao = agente.escolher_acao(estado)\n","            prox_estado, recompensa, terminou, info = env.executar_passo(acao)\n","            agente.aprender(estado, acao, recompensa, prox_estado)\n","            estado = prox_estado\n","            recompensa_total += recompensa\n","\n","        agente.epsilon = max(epsilon_minimo, agente.epsilon * taxa_decaimento_epsilon)\n","        recompensas.append(recompensa_total)\n","        if (episodio + 1) % 5000 == 0:\n","            recompensa_media = np.mean(recompensas[-5000:])\n","            print(f\"Epis贸dio {episodio + 1}/{episodios} - Recompensa M茅dia: {recompensa_media:.2f} - Epsilon: {agente.epsilon:.3f}\")\n","\n","    return agente, env\n","\n","if __name__ == '__main__':\n","    agente_treinado, env_treino = treinar_agente(episodios=100000)\n","\n","    estado = env_treino.reiniciar()\n","    agente_treinado.epsilon = 0\n","    terminou = False\n","    print(\"-\" * 20)\n","\n","    recompensa_total_avaliacao = 0\n","    info = \"\"\n","    while not terminou:\n","        env_treino.exibir_mapa()\n","        pos, tem_ouro, wumpus_vivo = estado\n","        print(f\"ltimo resultado: {info if info else 'In铆cio'}\")\n","        print(\"-\" * 20)\n","\n","        mapa_acoes = {0:'Cima', 1:'Esquerda', 2:'Baixo', 3:'Direita', 4:'Atirar', 5:'Pegar'}\n","        acao = agente_treinado.escolher_acao(estado)\n","\n","        prox_estado, recompensa, terminou, info = env_treino.executar_passo(acao)\n","        recompensa_total_avaliacao += recompensa\n","        estado = prox_estado\n","        time.sleep(1)\n","\n","    env_treino.exibir_mapa()\n","    print(f\"Recompensa total na avalia莽茫o: {recompensa_total_avaliacao}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uvDH7dmTMUwv","executionInfo":{"status":"ok","timestamp":1750766741125,"user_tz":180,"elapsed":40083,"user":{"displayName":"Jo茫o Pedro","userId":"13525678240025926987"}},"outputId":"d06dd3d6-40b6-4d9e-c124-1cc6d8b77ac4"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Epis贸dio 5000/100000 - Recompensa M茅dia: -686.81 - Epsilon: 0.701\n","Epis贸dio 10000/100000 - Recompensa M茅dia: -307.81 - Epsilon: 0.546\n","Epis贸dio 15000/100000 - Recompensa M茅dia: 71.83 - Epsilon: 0.425\n","Epis贸dio 20000/100000 - Recompensa M茅dia: 426.51 - Epsilon: 0.331\n","Epis贸dio 25000/100000 - Recompensa M茅dia: 662.42 - Epsilon: 0.258\n","Epis贸dio 30000/100000 - Recompensa M茅dia: 900.51 - Epsilon: 0.201\n","Epis贸dio 35000/100000 - Recompensa M茅dia: 1079.03 - Epsilon: 0.156\n","Epis贸dio 40000/100000 - Recompensa M茅dia: 1253.27 - Epsilon: 0.122\n","Epis贸dio 45000/100000 - Recompensa M茅dia: 1457.92 - Epsilon: 0.095\n","Epis贸dio 50000/100000 - Recompensa M茅dia: 1578.00 - Epsilon: 0.074\n","Epis贸dio 55000/100000 - Recompensa M茅dia: 1638.57 - Epsilon: 0.058\n","Epis贸dio 60000/100000 - Recompensa M茅dia: 1714.76 - Epsilon: 0.050\n","Epis贸dio 65000/100000 - Recompensa M茅dia: 1731.75 - Epsilon: 0.050\n","Epis贸dio 70000/100000 - Recompensa M茅dia: 1843.60 - Epsilon: 0.050\n","Epis贸dio 75000/100000 - Recompensa M茅dia: 1732.09 - Epsilon: 0.050\n","Epis贸dio 80000/100000 - Recompensa M茅dia: 1710.16 - Epsilon: 0.050\n","Epis贸dio 85000/100000 - Recompensa M茅dia: 1805.61 - Epsilon: 0.050\n","Epis贸dio 90000/100000 - Recompensa M茅dia: 1686.56 - Epsilon: 0.050\n","Epis贸dio 95000/100000 - Recompensa M茅dia: 1834.42 - Epsilon: 0.050\n","Epis贸dio 100000/100000 - Recompensa M茅dia: 1819.14 - Epsilon: 0.050\n","--------------------\n","\n","ㄢ锔锔葛仇\n","仇扳锔锔\n","ㄢ锔锔锔锔\n","锔斥锔\n","\n","ltimo resultado: In铆cio\n","--------------------\n","\n","ㄢ锔锔葛仇\n","仇扳锔锔\n","锔锔锔\n","ㄢ锔锔斥锔\n","\n","ltimo resultado: Moveu para (1, 2)\n","--------------------\n","\n","ㄢ锔锔葛仇\n","仇扳锔锔\n","ㄢ锔锔锔\n","ㄢ锔锔斥锔\n","\n","ltimo resultado: Moveu para (2, 2)\n","--------------------\n","\n","ㄢ锔锔葛仇\n","仇锔锔\n","ㄢ锔锔锔锔\n","ㄢ锔锔斥锔\n","\n","ltimo resultado: Moveu para (2, 3)\n","--------------------\n","\n","ㄢ锔锔葛仇\n","仇梆锔\n","ㄢ锔锔锔锔\n","ㄢ锔锔斥锔\n","\n","ltimo resultado: Moveu para (3, 3)\n","--------------------\n","\n","ㄢ锔锔仇\n","仇梆锔\n","ㄢ锔锔锔锔\n","ㄢ锔锔斥锔\n","\n","ltimo resultado: Matou o Wumpus\n","--------------------\n","\n","ㄢ锔锔仇\n","仇锔锔\n","ㄢ锔锔锔锔\n","ㄢ锔锔斥锔\n","\n","ltimo resultado: Moveu para (2, 3)\n","--------------------\n","\n","ㄢ锔锔仇\n","仇锔锔\n","ㄢ锔锔锔锔\n","ㄢ锔锔斥锔\n","\n","ltimo resultado: Pegou o ouro\n","--------------------\n","\n","ㄢ锔锔仇\n","斥锔锔锔\n","ㄢ锔锔锔\n","ㄢ锔锔斥锔\n","\n","ltimo resultado: Moveu para (2, 2)\n","--------------------\n","\n","ㄢ锔锔仇\n","斥锔锔锔\n","锔锔锔\n","ㄢ锔锔斥锔\n","\n","ltimo resultado: Moveu para (1, 2)\n","--------------------\n","\n","ㄢ锔锔仇\n","斥锔锔锔\n","ㄢ锔锔锔锔\n","锔斥锔\n","\n","Recompensa total na avalia莽茫o: 2081\n"]}]}]}