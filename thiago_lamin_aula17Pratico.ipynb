{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tVSCzaku1SNy",
        "outputId": "0aa85056-a7e1-4a59-f3e9-22edf2a3e881"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Percepções em [1,1]: []\n",
            "Percepções em [1,2]: []\n",
            "Percepções em [1,3]: ['Brisa']\n",
            "Percepções em [2,1]: ['Brisa']\n",
            "Percepções em [2,2]: ['Brisa']\n",
            "Percepções em [3,2]: ['Fedor', 'Brisa']\n",
            "É uma armadilha mortal em [3,1]? pit\n",
            "É uma armadilha mortal em [4,4]? None\n",
            "É uma armadilha mortal em [1,1]? None\n"
          ]
        }
      ],
      "source": [
        "#Criando o mundo\n",
        "import numpy as np\n",
        "import random\n",
        "class WumpusWorld:\n",
        "    def __init__(self):\n",
        "        # O mapa é uma grade 4x4. Coordenadas (coluna, linha) de 1 a 4.\n",
        "        # Definimos os elementos fixos do mapa.\n",
        "        self.grid_size = 4\n",
        "        self.wumpus_pos = (4, 2)  # Coluna 4, Linha 4\n",
        "        self.pit_positions = [(3, 1), (2, 3), (3, 4)]\n",
        "        self.gold_pos = (4, 4)\n",
        "        self.start_pos = (1, 1) # Agente sempre começa em [1,1]\n",
        "        self.agent_pos = self.start_pos\n",
        "        self.wumpus_alive = True\n",
        "        self.has_gold = False\n",
        "        self.arrows = 1 # Agente possui somente uma chance de atirar.\n",
        "\n",
        "        # Direções para movimento: Norte, Leste, Sul, Oeste\n",
        "        self.directions = {\n",
        "            'N': (0, 1),\n",
        "            'E': (1, 0),\n",
        "            'S': (0, -1),\n",
        "            'W': (-1, 0)\n",
        "        }\n",
        "        self.current_direction_idx = 0\n",
        "\n",
        "    def reset(self):\n",
        "        \"\"\"Redefine o ambiente para o estado inicial.\"\"\"\n",
        "        self.agent_pos = self.start_pos\n",
        "        self.wumpus_alive = True\n",
        "        self.has_gold = False\n",
        "        self.arrows = 1\n",
        "        self.current_direction_idx = 0\n",
        "        return self._get_state()\n",
        "\n",
        "    def _get_state(self):\n",
        "        \"\"\"\n",
        "        Retorna o estado atual do agente.\n",
        "        O estado pode ser uma tupla de (pos_x, pos_y, wumpus_alive, has_gold, current_direction_idx).\n",
        "        Para um grid 4x4, isso seria (1-4, 1-4, 0/1, 0/1, 0-3).\n",
        "        \"\"\"\n",
        "        return (self.agent_pos[0], self.agent_pos[1], int(self.wumpus_alive), int(self.has_gold), self.current_direction_idx)\n",
        "\n",
        "    def is_valid_position(self, x, y):\n",
        "        \"\"\"Verifica se uma posição (x, y) está dentro dos limites do mapa.\"\"\"\n",
        "        if 1 <= x <= self.grid_size and 1 <= y <= self.grid_size:\n",
        "            return True\n",
        "        else:\n",
        "            return False\n",
        "\n",
        "    def is_death_trap(self, x, y):\n",
        "        \"\"\"Verifica se a posição (x, y) é um abismo ou contém um Wumpus vivo.\"\"\"\n",
        "        if (x, y) in self.pit_positions:\n",
        "            return \"pit\" # Retorna \"pit\" se for um abismo\n",
        "        if (x, y) == self.wumpus_pos and self.wumpus_alive:\n",
        "            return \"wumpus\" # Retorna \"wumpus\" se for o Wumpus vivo\n",
        "        return None\n",
        "\n",
        "    def get_perceptions(self, agent_x, agent_y):\n",
        "        \"\"\"\n",
        "        Retorna as percepções do agente na posição (agent_x, agent_y).\n",
        "        As percepções são: cheiro (stench), brisa (breeze), brilho (glitter), choque (bump).\n",
        "        Um grito (scream) é global e acontece quando o Wumpus é morto.\n",
        "        \"\"\"\n",
        "        perceptions = []\n",
        "\n",
        "        # Verificar brilho (Glitter)\n",
        "        if (agent_x, agent_y) == self.gold_pos:\n",
        "            perceptions.append(\"Brilho\") # Agente percebe um brilho no compartimento onde o ouro está\n",
        "\n",
        "        # Verificar cheiro (Stench)\n",
        "        # Wumpus está no compartimento (wumpus_x, wumpus_y) ou em um compartimento adjacente\n",
        "        wumpus_x, wumpus_y = self.wumpus_pos\n",
        "        if ((agent_x == wumpus_x and abs(agent_y - wumpus_y) == 1) or\n",
        "            (agent_y == wumpus_y and abs(agent_x - wumpus_x) == 1) or\n",
        "            (agent_x, agent_y) == self.wumpus_pos):\n",
        "            perceptions.append(\"Fedor\") # No compartimento que contém o Wumpus e nos adjacentes, o agente percebe cheiro ruim\n",
        "\n",
        "        # Verificar brisa (Breeze)\n",
        "        # Abismos estão em pit_positions\n",
        "        for pit_x, pit_y in self.pit_positions:\n",
        "            if ((agent_x == pit_x and abs(agent_y - pit_y) == 1) or\n",
        "                (agent_y == pit_y and abs(agent_x - pit_x) == 1)):\n",
        "                perceptions.append(\"Brisa\") # Nos compartimentos adjacentes a um abismo, o agente percebe uma brisa\n",
        "                break # Uma brisa já é suficiente\n",
        "\n",
        "        return perceptions\n",
        "\n",
        "    def show_full_map(self):\n",
        "        \"\"\"\n",
        "        Imprime o mapa completo do Mundo do Wumpus no console.\n",
        "        'W': Wumpus\n",
        "        'P': Abismo (Pit)\n",
        "        'G': Ouro (Gold)\n",
        "        'S': Início (Start) / Agente\n",
        "        '.': Vazio\n",
        "        \"\"\"\n",
        "        print(\"\\n--- Mapa do Mundo do Wumpus ---\")\n",
        "        # Itera pelas linhas de cima para baixo (grid_size até 1)\n",
        "        for y in range(self.grid_size, 0, -1):\n",
        "            row_str = \"\"\n",
        "            for x in range(1, self.grid_size + 1):\n",
        "                cell_content = '.'\n",
        "                if (x, y) == self.wumpus_pos:\n",
        "                    cell_content = 'W'\n",
        "                elif (x, y) == self.gold_pos:\n",
        "                    cell_content = 'G'\n",
        "                elif (x, y) in self.pit_positions:\n",
        "                    cell_content = 'P'\n",
        "                elif (x, y) == self.start_pos:\n",
        "                    cell_content = 'S' # A posição inicial também pode ter um item, mas para este exemplo, o agente \"está\" ali.\n",
        "\n",
        "                row_str += f\"[{cell_content}] \"\n",
        "            print(row_str)\n",
        "        print(\"-------------------------------\\n\")\n",
        "    def step(self, action):\n",
        "          \"\"\"\n",
        "        Executa uma ação no ambiente e retorna (novo_estado, recompensa, done).\n",
        "        Ações:\n",
        "        0: Mover para frente\n",
        "        1: Virar à direita\n",
        "        2: Virar à esquerda\n",
        "        3: Pegar ouro\n",
        "        4: Atirar flecha\n",
        "        5: Sair da caverna (apenas em [1,1] com ouro)\n",
        "        Recompensas:\n",
        "        Local vazio: -1\n",
        "        Sentir brisa: -2\n",
        "        Sentir Fedor: -2\n",
        "        Sentir brilho: 10\n",
        "        Morrer pro wumpus: -100\n",
        "        Atirar: 20 #Apenas ganha se atira quando sente fedor\n",
        "        Matar o wumpus: 50\n",
        "        Sair da caverna: 100\n",
        "        Cair num buraco: -50\n",
        "        \"\"\"\n",
        "          reward = -1 # Recompensa base por cada ação, Local vazio: -1\n",
        "          done = False\n",
        "          (agent_x, agent_y) = self.agent_pos\n",
        "          (next_x, next_y) = (agent_x, agent_y)\n",
        "\n",
        "          # Direção atual do agente\n",
        "          current_dir_vec = list(self.directions.values())[self.current_direction_idx]\n",
        "\n",
        "          if action == 0:  # Mover para frente\n",
        "              next_x = agent_x + current_dir_vec[0]\n",
        "              next_y = agent_y + current_dir_vec[1]\n",
        "              if self.is_valid_position(next_x, next_y):\n",
        "                  self.agent_pos = (next_x, next_y)\n",
        "                  death_reason = self.is_death_trap(next_x, next_y)\n",
        "                  if death_reason == \"wumpus\":\n",
        "                      reward = -100 # Morrer pro wumpus: -100\n",
        "                      done = True\n",
        "                  elif death_reason == \"pit\":\n",
        "                      reward = -50 # Cair num buraco: -50\n",
        "                      done = True\n",
        "              else:\n",
        "                  pass\n",
        "          elif action == 1:  # Virar à direita\n",
        "              self.current_direction_idx = (self.current_direction_idx + 1) % 4\n",
        "          elif action == 2:  # Virar à esquerda\n",
        "              self.current_direction_idx = (self.current_direction_idx - 1 + 4) % 4 # Garante resultado positivo\n",
        "          elif action == 3:  # Pegar ouro\n",
        "              if self.agent_pos == self.gold_pos and not self.has_gold:\n",
        "                  self.has_gold = True\n",
        "                  # Recompensa por pegar ouro não é explícita aqui, mas é implicitamente incentivada\n",
        "                  # por permitir a ação 'Sair da caverna' com alta recompensa.\n",
        "              else:\n",
        "                  # Ação inválida ou ouro já pego. Punição leve.\n",
        "                  pass\n",
        "          elif action == 4:  # Atirar flecha\n",
        "              if self.arrows > 0:\n",
        "                  self.arrows -= 1\n",
        "                  current_perceptions = self.get_perceptions(agent_x, agent_y)\n",
        "                  if \"Fedor\" in current_perceptions:\n",
        "                    reward += 20\n",
        "                  # Calcular a posição alvo da flecha (um passo à frente)\n",
        "                  target_x = agent_x + current_dir_vec[0]\n",
        "                  target_y = agent_y + current_dir_vec[1]\n",
        "\n",
        "                  # Se o Wumpus estiver na célula adjacente na direção que o agente está virado:\n",
        "                  if self.wumpus_alive and (target_x, target_y) == self.wumpus_pos:\n",
        "                      self.wumpus_alive = False\n",
        "                      reward += 50 # Matar o wumpus: 50\n",
        "              else:\n",
        "                  # Ação inválida: Tentar atirar sem flechas. Não adiciona penalidade extra,\n",
        "                  # apenas o custo base de -1 pela ação já é suficiente para desincentivar.\n",
        "                  pass\n",
        "          elif action == 5:  # Sair da caverna\n",
        "              if self.agent_pos == self.start_pos and self.has_gold:\n",
        "                  reward = 100 # Sair da caverna: 100 (substitui a penalidade base)\n",
        "                  done = True\n",
        "              else:\n",
        "                  pass\n",
        "\n",
        "          return self._get_state(), reward, done\n",
        "if __name__ == \"__main__\": #Teste de percepções\n",
        "    world = WumpusWorld()\n",
        "\n",
        "    print(f\"Percepções em [1,1]: {world.get_perceptions(1, 1)}\")\n",
        "    print(f\"Percepções em [1,2]: {world.get_perceptions(1, 2)}\")\n",
        "    print(f\"Percepções em [1,3]: {world.get_perceptions(1, 3)}\") # Ouro\n",
        "    print(f\"Percepções em [2,1]: {world.get_perceptions(2, 1)}\")\n",
        "    print(f\"Percepções em [2,2]: {world.get_perceptions(2, 2)}\")\n",
        "    print(f\"Percepções em [3,2]: {world.get_perceptions(3, 2)}\") # Abismo\n",
        "\n",
        "    print(f\"É uma armadilha mortal em [3,1]? {world.is_death_trap(3, 1)}\") # Abismo\n",
        "    print(f\"É uma armadilha mortal em [4,4]? {world.is_death_trap(4, 4)}\") # Wumpus\n",
        "    print(f\"É uma armadilha mortal em [1,1]? {world.is_death_trap(1, 1)}\") # Início\n",
        "\n",
        "#Feito com auxilio do Gemini"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Mundo criado\n",
        "world.show_full_map()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o-G4_p6G3V61",
        "outputId": "b3638305-1c85-4bea-cc60-531b1070de3f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Mapa do Mundo do Wumpus ---\n",
            "[.] [.] [P] [G] \n",
            "[.] [P] [.] [.] \n",
            "[.] [.] [.] [W] \n",
            "[S] [.] [P] [.] \n",
            "-------------------------------\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class QLearningAgent:\n",
        "    def __init__(self, env, learning_rate=0.1, discount_factor=0.9, epsilon=1.0, epsilon_decay_rate=0.01, min_epsilon=0.01):\n",
        "        self.env = env\n",
        "        self.lr = learning_rate # Taxa de aprendizado (alpha)\n",
        "        self.gamma = discount_factor # Fator de desconto (gamma)\n",
        "        self.epsilon = epsilon # Taxa de exploração (epsilon)\n",
        "        self.epsilon_decay_rate = epsilon_decay_rate\n",
        "        self.min_epsilon = min_epsilon\n",
        "\n",
        "        # Ações: Mover Frente, Virar Direita, Virar Esquerda, Pegar Ouro, Atirar Flecha, Escalar\n",
        "        self.actions = [0, 1, 2, 3, 4, 5]\n",
        "        self.num_actions = len(self.actions)\n",
        "\n",
        "        self.q_table = {}\n",
        "\n",
        "    def _get_q_value(self, state, action):\n",
        "        \"\"\"Retorna o valor Q para um dado estado e ação, ou 0 se não existir.\"\"\"\n",
        "        return self.q_table.get((state, action), 0.0)\n",
        "\n",
        "    def choose_action(self, state):\n",
        "        \"\"\"Escolhe uma ação usando a política epsilon-greedy.\"\"\"\n",
        "        if random.uniform(0, 1) < self.epsilon:\n",
        "            return random.choice(self.actions) # Exploração: escolhe uma ação aleatória\n",
        "        else:\n",
        "            # Exploração com base na ação com o maior valor Q\n",
        "            q_values = [self._get_q_value(state, action) for action in self.actions]\n",
        "            max_q = max(q_values)\n",
        "            # Em caso de empate, escolha uma aleatoriamente entre as melhores\n",
        "            best_actions = [self.actions[i] for i, q in enumerate(q_values) if q == max_q]\n",
        "            return random.choice(best_actions)\n",
        "\n",
        "    def learn(self, state, action, reward, next_state, done):\n",
        "        \"\"\"Atualiza a tabela Q com base na experiência.\"\"\"\n",
        "        current_q = self._get_q_value(state, action)\n",
        "\n",
        "        if done:\n",
        "            target_q = reward\n",
        "        else:\n",
        "            # Encontra o valor Q máximo para o próximo estado\n",
        "            max_next_q = max([self._get_q_value(next_state, a) for a in self.actions])\n",
        "            target_q = reward + self.gamma * max_next_q\n",
        "\n",
        "        # Atualiza o valor Q\n",
        "        self.q_table[(state, action)] = current_q + self.lr * (target_q - current_q)\n",
        "\n",
        "    def decay_epsilon(self):\n",
        "        \"\"\"Diminui o valor de epsilon.\"\"\"\n",
        "        self.epsilon = max(self.min_epsilon, self.epsilon - self.epsilon_decay_rate)"
      ],
      "metadata": {
        "id": "gS5RUvEW5aXz"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Ele não encontrou uma saida com menos de 450 episodios\n",
        "#Assim, temos uma solução\n",
        "num_episodes = 450\n",
        "max_steps_per_episode = 200 # Limite de passos para evitar loops infinitos\n",
        "\n",
        "# Inicializar ambiente e agente\n",
        "world = WumpusWorld()\n",
        "agent = QLearningAgent(world)\n",
        "\n",
        "# Armazenar recompensas para plotagem ou análise\n",
        "rewards_per_episode = []\n",
        "\n",
        "for episode in range(num_episodes):\n",
        "    state = world.reset() # Redefine o ambiente para o início de cada episódio\n",
        "    total_reward = 0\n",
        "    done = False\n",
        "    step = 0\n",
        "\n",
        "    while not done and step < max_steps_per_episode:\n",
        "        action = agent.choose_action(state)\n",
        "        next_state, reward, done = world.step(action)\n",
        "        agent.learn(state, action, reward, next_state, done)\n",
        "\n",
        "        state = next_state\n",
        "        total_reward += reward\n",
        "        step += 1\n",
        "\n",
        "    rewards_per_episode.append(total_reward)\n",
        "    agent.decay_epsilon()\n",
        "\n",
        "    if (episode + 1) % 1000 == 0:\n",
        "        print(f\"Episódio {episode + 1}/{num_episodes}, Recompensa Total: {total_reward}, Epsilon: {agent.epsilon:.4f}\")\n",
        "\n",
        "# Exemplo de como o agente se comportaria após o treinamento (política ótima)\n",
        "print(\"\\n--- Testando o agente treinado ---\")\n",
        "state = world.reset()\n",
        "done = False\n",
        "total_reward = 0\n",
        "path = [world.agent_pos]\n",
        "test_steps = 0\n",
        "\n",
        "while not done and test_steps < max_steps_per_episode * 2: # Mais passos para o teste\n",
        "    # Escolhe a ação com o maior Q-value (sem exploração)\n",
        "    q_values = [agent._get_q_value(state, action) for action in agent.actions]\n",
        "    if not q_values: # Caso extremo onde o estado-ação não foi explorado\n",
        "        action = random.choice(agent.actions)\n",
        "    else:\n",
        "        max_q = max(q_values)\n",
        "        best_actions = [agent.actions[i] for i, q in enumerate(q_values) if q == max_q]\n",
        "        action = random.choice(best_actions) # Em caso de empate\n",
        "\n",
        "    next_state, reward, done = world.step(action)\n",
        "    total_reward += reward\n",
        "    state = next_state\n",
        "    path.append(world.agent_pos)\n",
        "    test_steps += 1\n",
        "\n",
        "    if done:\n",
        "        death_reason = world.is_death_trap(world.agent_pos[0], world.agent_pos[1])\n",
        "        if world.agent_pos == world.start_pos and world.has_gold:\n",
        "            print(\"Agente encontrou o ouro e escapou!\")\n",
        "        elif death_reason == \"wumpus\":\n",
        "            print(\"Agente morreu para o Wumpus!\")\n",
        "        elif death_reason == \"pit\":\n",
        "            print(\"Agente caiu em um buraco!\")\n",
        "        else: # Outras condições de 'done' (e.g., max_steps)\n",
        "            print(\"Episódio de teste terminado por limite de passos ou condição desconhecida.\")\n",
        "        break\n",
        "\n",
        "print(f\"Recompensa Final no Teste: {total_reward}\")\n",
        "print(f\"Caminho percorrido: {path}\")\n",
        "print(f\"Tamanho da Q-table: {len(agent.q_table)}\")\n",
        "\n",
        "# Você pode plotar rewards_per_episode para visualizar o aprendizado\n",
        "# import matplotlib.pyplot as plt\n",
        "# plt.plot(rewards_per_episode)\n",
        "# plt.xlabel(\"Episódios\")\n",
        "# plt.ylabel(\"Recompensa Total\")\n",
        "# plt.title(\"Recompensa por Episódio no Mundo do Wumpus\")\n",
        "# plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h_tzMSMS-6Ih",
        "outputId": "2eaf29b9-a758-4eaf-da20-f1ef1c8e915a"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Testando o agente treinado ---\n",
            "Agente encontrou o ouro e escapou!\n",
            "Recompensa Final no Teste: 150\n",
            "Caminho percorrido: [(1, 1), (1, 2), (1, 2), (2, 2), (3, 2), (3, 2), (4, 2), (4, 2), (4, 3), (4, 4), (4, 4), (4, 4), (4, 4), (4, 3), (4, 2), (4, 2), (3, 2), (2, 2), (1, 2), (1, 2), (1, 1), (1, 1)]\n",
            "Tamanho da Q-table: 796\n"
          ]
        }
      ]
    }
  ]
}