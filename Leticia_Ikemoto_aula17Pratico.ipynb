{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a5fa143a",
   "metadata": {},
   "source": [
    "# Mundo de Wumpus\n",
    "\n",
    "    primeiro pegaremos o algoritmo implementado das aulas passadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a09a0bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from collections import deque\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "b899f555",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicialização do Q-table\n",
    "\n",
    "# Definindo o ambiente do mundo de Wumpus\n",
    "# 0 = vazio, 1 = poço, 2 = Wumpus, 3 = ouro\n",
    "# O agente começa na posição (0, 0)\n",
    "\n",
    "ambiente = [\n",
    "    [0, 0, 0, 0],\n",
    "    [0, 0, 0, 0],\n",
    "    [2, 0, 1, 0],\n",
    "    [0, 0, 1, 3]\n",
    "]\n",
    "\n",
    "acoes = ['N', 'S', 'L', 'O', 'atirar_N', 'atirar_S', 'atirar_L', 'atirar_O']\n",
    "q_table = {}\n",
    "gamma = 0.8\n",
    "alpha = 0.8\n",
    "epsilon = 0.2\n",
    "tamanho = 4\n",
    "episodios = 1000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "49318f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def estado_para_str(pos):\n",
    "    return str(pos[0]) + '_' + str(pos[1])\n",
    "\n",
    "def get_action(state):\n",
    "    if state not in q_table:\n",
    "        q_table[state] = {a: 0 for a in acoes}\n",
    "    if random.random() < epsilon:\n",
    "        return random.choice(acoes)\n",
    "    return max(q_table[state], key=q_table[state].get)\n",
    "\n",
    "def mover(pos, acao):\n",
    "    x, y = pos\n",
    "    if acao == 'N':\n",
    "        x = max(0, x - 1)\n",
    "    elif acao == 'S':\n",
    "        x = min(tamanho - 1, x + 1)\n",
    "    elif acao == 'L':\n",
    "        y = min(tamanho - 1, y + 1)\n",
    "    elif acao == 'O':\n",
    "        y = max(0, y - 1)\n",
    "    return (x, y)\n",
    "\n",
    "def atirar(pos, direcao):\n",
    "    x, y = pos\n",
    "    if direcao == 'N':\n",
    "        for i in range(x-1, -1, -1):\n",
    "            if ambiente[i][y] == 2:\n",
    "                ambiente[i][y] = 0\n",
    "                return True\n",
    "    elif direcao == 'S':\n",
    "        for i in range(x+1, tamanho):\n",
    "            if ambiente[i][y] == 2:\n",
    "                ambiente[i][y] = 0\n",
    "                return True\n",
    "    elif direcao == 'L':\n",
    "        for j in range(y+1, tamanho):\n",
    "            if ambiente[x][j] == 2:\n",
    "                ambiente[x][j] = 0\n",
    "                return True\n",
    "    elif direcao == 'O':\n",
    "        for j in range(y-1, -1, -1):\n",
    "            if ambiente[x][j] == 2:\n",
    "                ambiente[x][j] = 0\n",
    "                return True\n",
    "    return False\n",
    "\n",
    "def get_evento(celula):\n",
    "    if celula == 1:\n",
    "        return \"poco\"\n",
    "    elif celula == 2:\n",
    "        return \"wumpus\"\n",
    "    elif celula == 3:\n",
    "        return \"ouro\"\n",
    "    # As condições de \"flecha\" e \"parede\" foram removidas pois estavam incorretas.\n",
    "    # A recompensa por movimento normal (passo) será tratada no loop.\n",
    "    return \"nada\"\n",
    "\n",
    "def get_recompensa(evento):\n",
    "    if evento == \"ouro\":\n",
    "        return 100\n",
    "    elif evento == \"wumpus\" or evento == \"poco\":\n",
    "        return -100\n",
    "    elif evento == \"errou flecha\" or evento == \"parede\":\n",
    "        return -10\n",
    "    return -1\n",
    "\n",
    "def update_q_table(state, action, reward, next_state):\n",
    "    if next_state not in q_table:\n",
    "        q_table[next_state] = {a: 0 for a in acoes}\n",
    "    max_q_next = max(q_table[next_state].values())\n",
    "    q_table[state][action] = q_table[state][action] + alpha * (reward + gamma * max_q_next - q_table[state][action])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "e6b182c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treinamento concluído!\n"
     ]
    }
   ],
   "source": [
    "# Treinamento (Versão Final Corrigida)\n",
    "for ep in range(episodios):\n",
    "    pos = (0, 0)\n",
    "    state = estado_para_str(pos)\n",
    "    # Reseta o ambiente a cada novo episódio\n",
    "    ambiente_episodio = [row[:] for row in ambiente]\n",
    "    wumpus_morto = False\n",
    "\n",
    "    for step in range(50):\n",
    "        if state not in q_table:\n",
    "            q_table[state] = {a: 0 for a in acoes}\n",
    "\n",
    "        action = get_action(state)\n",
    "        recompensa = 0\n",
    "\n",
    "        # --- Lógica para ATIRAR ---\n",
    "        if 'atirar' in action:\n",
    "            direcao = action.split('_')[1]\n",
    "            acertou = atirar(pos, direcao)\n",
    "\n",
    "            if acertou:\n",
    "                recompensa = 50  # Recompensa positiva por matar o Wumpus\n",
    "                wumpus_morto = True\n",
    "            else:\n",
    "                recompensa = -10 # Penalidade por errar a flecha\n",
    "\n",
    "            # Ao atirar, o agente não se move\n",
    "            prox_state = estado_para_str(pos)\n",
    "            update_q_table(state, action, recompensa, prox_state)\n",
    "\n",
    "        # --- Lógica para MOVER ---\n",
    "        else:\n",
    "            nova_pos = mover(pos, action)\n",
    "            # Usa o ambiente do episódio para obter o evento\n",
    "            evento = get_evento(ambiente_episodio[nova_pos[0]][nova_pos[1]])\n",
    "\n",
    "            # Se o wumpus foi morto, a casa dele é segura (recompensa de passo)\n",
    "            if evento == \"wumpus\" and wumpus_morto:\n",
    "                 recompensa = -1\n",
    "            else:\n",
    "                 recompensa = get_recompensa(evento)\n",
    "\n",
    "            prox_state = estado_para_str(nova_pos)\n",
    "            update_q_table(state, action, recompensa, prox_state)\n",
    "            pos = nova_pos # Atualiza a posição somente após mover\n",
    "\n",
    "        state = prox_state\n",
    "\n",
    "        # Condição de término do episódio\n",
    "        evento_final = get_evento(ambiente_episodio[pos[0]][pos[1]])\n",
    "        if evento_final == \"ouro\" or (evento_final == \"wumpus\" and not wumpus_morto) or evento_final == \"poco\":\n",
    "            break\n",
    "\n",
    "print(\"Treinamento concluído!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "22f2866a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execução do agente treinado:\n",
      "Caminho seguido pelo agente: [(0, 0), (1, 0), (1, 1), (1, 2), (1, 3), (2, 3), (3, 3)]\n"
     ]
    }
   ],
   "source": [
    "# Bloco de Execução Final\n",
    "print(\"Execução do agente treinado:\")\n",
    "pos = (0, 0)\n",
    "state = estado_para_str(pos)\n",
    "path = [pos]\n",
    "wumpus_morto = False # Supondo que você queira rastrear isso também na execução\n",
    "\n",
    "# --- INÍCIO DA CORREÇÃO ---\n",
    "# Verifica a condição inicial ANTES de iniciar o loop de ações\n",
    "evento_inicial = get_evento(ambiente[pos[0]][pos[1]])\n",
    "if evento_inicial == \"ouro\":\n",
    "    print(\"Ouro encontrado na posição inicial!\")\n",
    "else:\n",
    "    # O loop só é executado se o agente não começar no ouro\n",
    "    for step in range(20):\n",
    "        if state not in q_table:\n",
    "            q_table[state] = {a: 0 for a in acoes}\n",
    "        \n",
    "        # Escolhe a melhor ação aprendida (sem explorar)\n",
    "        action = max(q_table[state], key=q_table[state].get)\n",
    "\n",
    "        # Se a ação for atirar\n",
    "        if 'atirar' in action:\n",
    "            if not wumpus_morto:\n",
    "                acertou = atirar(pos, action.split('_')[1])\n",
    "                if acertou:\n",
    "                    print(\"Agente atirou e matou o Wumpus!\")\n",
    "                    wumpus_morto = True\n",
    "            # O agente não se move ao atirar\n",
    "            nova_pos = pos\n",
    "        # Se a ação for mover\n",
    "        else:\n",
    "            nova_pos = mover(pos, action)\n",
    "        \n",
    "        path.append(nova_pos)\n",
    "        pos = nova_pos\n",
    "        state = estado_para_str(pos)\n",
    "\n",
    "        evento = get_evento(ambiente[pos[0]][pos[1]])\n",
    "        if (evento == \"wumpus\" and not wumpus_morto) or evento == \"poco\" or evento == \"ouro\":\n",
    "            break\n",
    "# --- FIM DA CORREÇÃO ---\n",
    "\n",
    "print(\"Caminho seguido pelo agente:\", path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a446082",
   "metadata": {},
   "source": [
    "No algoritmo atual, estabelecemos as recompensas e o melhor camminho treinando.\n",
    "Caso seja utilizado o action = get_action(state), teremos resultados mais variados e os diferentes outputs de caminhos seguidos pelo agente após o treinamento acontecem porque o algoritmo Q-learning, por padrão, envolve exploração estocástica (aleatória). Isso significa que mesmo após o treinamento, ainda há uma probabilidade do agente explorar — ou seja, tomar ações não ótimas de propósito."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73ad333a",
   "metadata": {},
   "source": [
    "## Fontes:\n",
    "- continuam sendo as mesmas fontes do codigo antigo\n",
    "- https://youtu.be/t_bTU7OqmnA?si=mMCHEi3ArZRsTZSp\n",
    "- https://github.com/nrupeshsurya/Wumpus-world/blob/master/2017B2A70767G_NRUPESH.py\n",
    "- chatGPT\n",
    "- Gemini\n",
    "- slides da aula"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
